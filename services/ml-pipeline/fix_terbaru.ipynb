{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memeriksa folder1: dataset/training\\Rendah\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Abdul Aziz_clip1.mp4\n",
      "Extraction complete for Abdul Aziz_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Abdul Aziz_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Abdul Aziz_clip2.mp4\n",
      "Extraction complete for Abdul Aziz_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Abdul Aziz_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Abian Agung Shafiqri_clip1.mp4\n",
      "Extraction complete for Abian Agung Shafiqri_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Abian Agung Shafiqri_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Abian Agung Shafiqri_clip2.mp4\n",
      "Extraction complete for Abian Agung Shafiqri_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Abian Agung Shafiqri_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Abima Fadricho S_clip1.mp4\n",
      "Extraction complete for Abima Fadricho S_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Abima Fadricho S_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Abima Fadricho S_clip2.mp4\n",
      "Extraction complete for Abima Fadricho S_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Abima Fadricho S_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Ahmad Mumtaz Haris_clip1.mp4\n",
      "Extraction complete for Ahmad Mumtaz Haris_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Ahmad Mumtaz Haris_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Ahmad Mumtaz Haris_clip2.mp4\n",
      "Extraction complete for Ahmad Mumtaz Haris_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Ahmad Mumtaz Haris_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Aji Hamdani_clip1.mp4\n",
      "Extraction complete for Aji Hamdani_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Aji Hamdani_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Aji Hamdani_clip2.mp4\n",
      "Extraction complete for Aji Hamdani_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Aji Hamdani_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Aleron Tsafif Rakha_clip1.mp4\n",
      "Extraction complete for Aleron Tsafif Rakha_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Aleron Tsafif Rakha_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Aleron Tsafif Rakha_clip2.mp4\n",
      "Extraction complete for Aleron Tsafif Rakha_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Aleron Tsafif Rakha_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Athiyan Aqil Muhammad_clip1.mp4\n",
      "Extraction complete for Athiyan Aqil Muhammad_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Athiyan Aqil Muhammad_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Athiyan Aqil Muhammad_clip2.mp4\n",
      "Extraction complete for Athiyan Aqil Muhammad_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Athiyan Aqil Muhammad_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Cyndu Fathur Rahman_clip1.mp4\n",
      "Extraction complete for Cyndu Fathur Rahman_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Cyndu Fathur Rahman_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Cyndu Fathur Rahman_clip2.mp4\n",
      "Extraction complete for Cyndu Fathur Rahman_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Cyndu Fathur Rahman_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Dany Fatihul Ihsan_clip1.mp4\n",
      "Extraction complete for Dany Fatihul Ihsan_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Dany Fatihul Ihsan_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Dany Fatihul Ihsan_clip2.mp4\n",
      "Extraction complete for Dany Fatihul Ihsan_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Dany Fatihul Ihsan_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Dennis Parulian Panjaitan_clip1.mp4\n",
      "Extraction complete for Dennis Parulian Panjaitan_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Dennis Parulian Panjaitan_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Dennis Parulian Panjaitan_clip2.mp4\n",
      "Extraction complete for Dennis Parulian Panjaitan_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Dennis Parulian Panjaitan_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Dido Imam Padmanegara_clip1.mp4\n",
      "Extraction complete for Dido Imam Padmanegara_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Dido Imam Padmanegara_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Dido Imam Padmanegara_clip2.mp4\n",
      "Extraction complete for Dido Imam Padmanegara_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Dido Imam Padmanegara_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Diva Aji Kurniawan_clip1.mp4\n",
      "Extraction complete for Diva Aji Kurniawan_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Diva Aji Kurniawan_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Diva Aji Kurniawan_clip2.mp4\n",
      "Extraction complete for Diva Aji Kurniawan_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Diva Aji Kurniawan_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Fadly Nugraha Jati_clip1.mp4\n",
      "Extraction complete for Fadly Nugraha Jati_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Fadly Nugraha Jati_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Fadly Nugraha Jati_clip2.mp4\n",
      "Extraction complete for Fadly Nugraha Jati_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Fadly Nugraha Jati_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Fransiscus Farrel E_clip1.mp4\n",
      "Extraction complete for Fransiscus Farrel E_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Fransiscus Farrel E_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Fransiscus Farrel E_clip2.mp4\n",
      "Extraction complete for Fransiscus Farrel E_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Fransiscus Farrel E_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Gaco Razan Kamil_clip1.mp4\n",
      "Extraction complete for Gaco Razan Kamil_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Gaco Razan Kamil_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Gaco Razan Kamil_clip2.mp4\n",
      "Extraction complete for Gaco Razan Kamil_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Gaco Razan Kamil_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Habibatul Mustofa_clip1.mp4\n",
      "Extraction complete for Habibatul Mustofa_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Habibatul Mustofa_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Habibatul Mustofa_clip2.mp4\n",
      "Extraction complete for Habibatul Mustofa_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Habibatul Mustofa_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Hafizh Muhammad R_clip1.mp4\n",
      "Extraction complete for Hafizh Muhammad R_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Hafizh Muhammad R_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Hafizh Muhammad R_clip2.mp4\n",
      "Extraction complete for Hafizh Muhammad R_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Hafizh Muhammad R_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Hilmi Irfan Naafi'udin_clip1.mp4\n",
      "Extraction complete for Hilmi Irfan Naafi'udin_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Hilmi Irfan Naafi'udin_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Hilmi Irfan Naafi'udin_clip2.mp4\n",
      "Extraction complete for Hilmi Irfan Naafi'udin_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Hilmi Irfan Naafi'udin_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Joyo Sugito_clip1.mp4\n",
      "Extraction complete for Joyo Sugito_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Joyo Sugito_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Joyo Sugito_clip2.mp4\n",
      "Extraction complete for Joyo Sugito_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Joyo Sugito_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\M Cholilur Rokhman_clip1.mp4\n",
      "Extraction complete for M Cholilur Rokhman_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\M Cholilur Rokhman_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\M Cholilur Rokhman_clip2.mp4\n",
      "Extraction complete for M Cholilur Rokhman_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\M Cholilur Rokhman_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\M Daffa Wijaya_clip1.mp4\n",
      "Extraction complete for M Daffa Wijaya_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\M Daffa Wijaya_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\M Daffa Wijaya_clip2.mp4\n",
      "Extraction complete for M Daffa Wijaya_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\M Daffa Wijaya_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\M Nurul Mustofa_clip1.mp4\n",
      "Extraction complete for M Nurul Mustofa_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\M Nurul Mustofa_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\M Nurul Mustofa_clip2.mp4\n",
      "Extraction complete for M Nurul Mustofa_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\M Nurul Mustofa_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\M Ridlo Febrio Putra_clip1.mp4\n",
      "Extraction complete for M Ridlo Febrio Putra_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\M Ridlo Febrio Putra_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\M Ridlo Febrio Putra_clip2.mp4\n",
      "Extraction complete for M Ridlo Febrio Putra_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\M Ridlo Febrio Putra_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Moch Reynal Silva Baktiar_clip1.mp4\n",
      "Extraction complete for Moch Reynal Silva Baktiar_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Moch Reynal Silva Baktiar_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Moch Reynal Silva Baktiar_clip2.mp4\n",
      "Extraction complete for Moch Reynal Silva Baktiar_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Moch Reynal Silva Baktiar_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Muhammad Fathurrozak Al Qoroni_clip1.mp4\n",
      "Extraction complete for Muhammad Fathurrozak Al Qoroni_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Muhammad Fathurrozak Al Qoroni_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Muhammad Fathurrozak Al Qoroni_clip2.mp4\n",
      "Extraction complete for Muhammad Fathurrozak Al Qoroni_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Muhammad Fathurrozak Al Qoroni_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Muhammad Rizky Fauzi_clip1.mp4\n",
      "Extraction complete for Muhammad Rizky Fauzi_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Muhammad Rizky Fauzi_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Muhammad Rizky Fauzi_clip2.mp4\n",
      "Extraction complete for Muhammad Rizky Fauzi_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Muhammad Rizky Fauzi_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Mulki Hakim_clip1.mp4\n",
      "Extraction complete for Mulki Hakim_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Mulki Hakim_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Mulki Hakim_clip2.mp4\n",
      "Extraction complete for Mulki Hakim_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Mulki Hakim_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Muzzarina Khaira Akbar_clip1.mp4\n",
      "Extraction complete for Muzzarina Khaira Akbar_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Muzzarina Khaira Akbar_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Muzzarina Khaira Akbar_clip2.mp4\n",
      "Extraction complete for Muzzarina Khaira Akbar_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Muzzarina Khaira Akbar_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Raihan Fazzaufa Rasendriya_clip1.mp4\n",
      "Extraction complete for Raihan Fazzaufa Rasendriya_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Raihan Fazzaufa Rasendriya_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Raihan Fazzaufa Rasendriya_clip2.mp4\n",
      "Extraction complete for Raihan Fazzaufa Rasendriya_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Raihan Fazzaufa Rasendriya_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Rio Bagas Hendrawan_clip1.mp4\n",
      "Extraction complete for Rio Bagas Hendrawan_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Rio Bagas Hendrawan_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Rio Bagas Hendrawan_clip2.mp4\n",
      "Extraction complete for Rio Bagas Hendrawan_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Rio Bagas Hendrawan_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Sukma Bagus Wahasjuika_clip1.mp4\n",
      "Extraction complete for Sukma Bagus Wahasjuika_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Sukma Bagus Wahasjuika_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Sukma Bagus Wahasjuika_clip2.mp4\n",
      "Extraction complete for Sukma Bagus Wahasjuika_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Sukma Bagus Wahasjuika_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Syahrul Bhudi Ferdiansyah_clip1.mp4\n",
      "Extraction complete for Syahrul Bhudi Ferdiansyah_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Syahrul Bhudi Ferdiansyah_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Syahrul Bhudi Ferdiansyah_clip2.mp4\n",
      "Extraction complete for Syahrul Bhudi Ferdiansyah_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Syahrul Bhudi Ferdiansyah_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Tirta Nurrochman B P_clip1.mp4\n",
      "Extraction complete for Tirta Nurrochman B P_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Tirta Nurrochman B P_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Tirta Nurrochman B P_clip2.mp4\n",
      "Extraction complete for Tirta Nurrochman B P_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Tirta Nurrochman B P_clip2\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Zaki Lazuardi Fersya P_clip1.mp4\n",
      "Extraction complete for Zaki Lazuardi Fersya P_clip1.mp4, saved 260 frames to dataset/training_images\\Rendah\\Zaki Lazuardi Fersya P_clip1\n",
      "Menemukan file .MP4: dataset/training\\Rendah\\Zaki Lazuardi Fersya P_clip2.mp4\n",
      "Extraction complete for Zaki Lazuardi Fersya P_clip2.mp4, saved 260 frames to dataset/training_images\\Rendah\\Zaki Lazuardi Fersya P_clip2\n",
      "Memeriksa folder1: dataset/training\\Sangat Rendah\n",
      "Menemukan file .MP4: dataset/training\\Sangat Rendah\\Afrizal Dwi S_clip1.mp4\n",
      "Extraction complete for Afrizal Dwi S_clip1.mp4, saved 260 frames to dataset/training_images\\Sangat Rendah\\Afrizal Dwi S_clip1\n",
      "Menemukan file .MP4: dataset/training\\Sangat Rendah\\Afrizal Dwi S_clip2.mp4\n",
      "Extraction complete for Afrizal Dwi S_clip2.mp4, saved 260 frames to dataset/training_images\\Sangat Rendah\\Afrizal Dwi S_clip2\n",
      "Menemukan file .MP4: dataset/training\\Sangat Rendah\\Andreagazy Iza A_clip1.mp4\n",
      "Extraction complete for Andreagazy Iza A_clip1.mp4, saved 260 frames to dataset/training_images\\Sangat Rendah\\Andreagazy Iza A_clip1\n",
      "Menemukan file .MP4: dataset/training\\Sangat Rendah\\Andreagazy Iza A_clip2.mp4\n",
      "Extraction complete for Andreagazy Iza A_clip2.mp4, saved 260 frames to dataset/training_images\\Sangat Rendah\\Andreagazy Iza A_clip2\n",
      "Menemukan file .MP4: dataset/training\\Sangat Rendah\\Fajar Bayu Kusuma_clip1.mp4\n",
      "Extraction complete for Fajar Bayu Kusuma_clip1.mp4, saved 260 frames to dataset/training_images\\Sangat Rendah\\Fajar Bayu Kusuma_clip1\n",
      "Menemukan file .MP4: dataset/training\\Sangat Rendah\\Fajar Bayu Kusuma_clip2.mp4\n",
      "Extraction complete for Fajar Bayu Kusuma_clip2.mp4, saved 260 frames to dataset/training_images\\Sangat Rendah\\Fajar Bayu Kusuma_clip2\n",
      "Menemukan file .MP4: dataset/training\\Sangat Rendah\\Irshandy Aditya Wicaksana_clip1.mp4\n",
      "Extraction complete for Irshandy Aditya Wicaksana_clip1.mp4, saved 260 frames to dataset/training_images\\Sangat Rendah\\Irshandy Aditya Wicaksana_clip1\n",
      "Menemukan file .MP4: dataset/training\\Sangat Rendah\\Irshandy Aditya Wicaksana_clip2.mp4\n",
      "Extraction complete for Irshandy Aditya Wicaksana_clip2.mp4, saved 260 frames to dataset/training_images\\Sangat Rendah\\Irshandy Aditya Wicaksana_clip2\n",
      "Menemukan file .MP4: dataset/training\\Sangat Rendah\\M Paksi Satrio_clip1.mp4\n",
      "Extraction complete for M Paksi Satrio_clip1.mp4, saved 260 frames to dataset/training_images\\Sangat Rendah\\M Paksi Satrio_clip1\n",
      "Menemukan file .MP4: dataset/training\\Sangat Rendah\\M Paksi Satrio_clip2.mp4\n",
      "Extraction complete for M Paksi Satrio_clip2.mp4, saved 260 frames to dataset/training_images\\Sangat Rendah\\M Paksi Satrio_clip2\n",
      "Menemukan file .MP4: dataset/training\\Sangat Rendah\\Nanda Putra Khamdani_clip1.mp4\n",
      "Extraction complete for Nanda Putra Khamdani_clip1.mp4, saved 260 frames to dataset/training_images\\Sangat Rendah\\Nanda Putra Khamdani_clip1\n",
      "Menemukan file .MP4: dataset/training\\Sangat Rendah\\Nanda Putra Khamdani_clip2.mp4\n",
      "Extraction complete for Nanda Putra Khamdani_clip2.mp4, saved 260 frames to dataset/training_images\\Sangat Rendah\\Nanda Putra Khamdani_clip2\n",
      "Menemukan file .MP4: dataset/training\\Sangat Rendah\\Rama Pramudhita Bhaskara_clip1.mp4\n",
      "Extraction complete for Rama Pramudhita Bhaskara_clip1.mp4, saved 260 frames to dataset/training_images\\Sangat Rendah\\Rama Pramudhita Bhaskara_clip1\n",
      "Menemukan file .MP4: dataset/training\\Sangat Rendah\\Rama Pramudhita Bhaskara_clip2.mp4\n",
      "Extraction complete for Rama Pramudhita Bhaskara_clip2.mp4, saved 260 frames to dataset/training_images\\Sangat Rendah\\Rama Pramudhita Bhaskara_clip2\n",
      "Menemukan file .MP4: dataset/training\\Sangat Rendah\\Rendy Putra Kusuma_clip1.mp4\n",
      "Extraction complete for Rendy Putra Kusuma_clip1.mp4, saved 260 frames to dataset/training_images\\Sangat Rendah\\Rendy Putra Kusuma_clip1\n",
      "Menemukan file .MP4: dataset/training\\Sangat Rendah\\Rendy Putra Kusuma_clip2.mp4\n",
      "Extraction complete for Rendy Putra Kusuma_clip2.mp4, saved 260 frames to dataset/training_images\\Sangat Rendah\\Rendy Putra Kusuma_clip2\n",
      "Menemukan file .MP4: dataset/training\\Sangat Rendah\\Septa Purna Surya_clip1.mp4\n",
      "Extraction complete for Septa Purna Surya_clip1.mp4, saved 260 frames to dataset/training_images\\Sangat Rendah\\Septa Purna Surya_clip1\n",
      "Menemukan file .MP4: dataset/training\\Sangat Rendah\\Septa Purna Surya_clip2.mp4\n",
      "Extraction complete for Septa Purna Surya_clip2.mp4, saved 260 frames to dataset/training_images\\Sangat Rendah\\Septa Purna Surya_clip2\n",
      "Memeriksa folder1: dataset/training\\Sangat Tinggi\n",
      "Menemukan file .MP4: dataset/training\\Sangat Tinggi\\Achmad Mufid_clip1.mp4\n",
      "Extraction complete for Achmad Mufid_clip1.mp4, saved 260 frames to dataset/training_images\\Sangat Tinggi\\Achmad Mufid_clip1\n",
      "Menemukan file .MP4: dataset/training\\Sangat Tinggi\\Achmad Mufid_clip2.mp4\n",
      "Extraction complete for Achmad Mufid_clip2.mp4, saved 260 frames to dataset/training_images\\Sangat Tinggi\\Achmad Mufid_clip2\n",
      "Menemukan file .MP4: dataset/training\\Sangat Tinggi\\Daffa Yudisa A_clip1.mp4\n",
      "Extraction complete for Daffa Yudisa A_clip1.mp4, saved 260 frames to dataset/training_images\\Sangat Tinggi\\Daffa Yudisa A_clip1\n",
      "Menemukan file .MP4: dataset/training\\Sangat Tinggi\\Daffa Yudisa A_clip2.mp4\n",
      "Extraction complete for Daffa Yudisa A_clip2.mp4, saved 260 frames to dataset/training_images\\Sangat Tinggi\\Daffa Yudisa A_clip2\n",
      "Menemukan file .MP4: dataset/training\\Sangat Tinggi\\Dika Dwi Alamsyah_clip1.mp4\n",
      "Extraction complete for Dika Dwi Alamsyah_clip1.mp4, saved 260 frames to dataset/training_images\\Sangat Tinggi\\Dika Dwi Alamsyah_clip1\n",
      "Menemukan file .MP4: dataset/training\\Sangat Tinggi\\Dika Dwi Alamsyah_clip2.mp4\n",
      "Extraction complete for Dika Dwi Alamsyah_clip2.mp4, saved 260 frames to dataset/training_images\\Sangat Tinggi\\Dika Dwi Alamsyah_clip2\n",
      "Menemukan file .MP4: dataset/training\\Sangat Tinggi\\Febiola Lidya S_clip1.mp4\n",
      "Extraction complete for Febiola Lidya S_clip1.mp4, saved 260 frames to dataset/training_images\\Sangat Tinggi\\Febiola Lidya S_clip1\n",
      "Menemukan file .MP4: dataset/training\\Sangat Tinggi\\Febiola Lidya S_clip2.mp4\n",
      "Extraction complete for Febiola Lidya S_clip2.mp4, saved 260 frames to dataset/training_images\\Sangat Tinggi\\Febiola Lidya S_clip2\n",
      "Menemukan file .MP4: dataset/training\\Sangat Tinggi\\Maulana Arya Putra Nugraha_clip1.mp4\n",
      "Extraction complete for Maulana Arya Putra Nugraha_clip1.mp4, saved 260 frames to dataset/training_images\\Sangat Tinggi\\Maulana Arya Putra Nugraha_clip1\n",
      "Menemukan file .MP4: dataset/training\\Sangat Tinggi\\Maulana Arya Putra Nugraha_clip2.mp4\n",
      "Extraction complete for Maulana Arya Putra Nugraha_clip2.mp4, saved 260 frames to dataset/training_images\\Sangat Tinggi\\Maulana Arya Putra Nugraha_clip2\n",
      "Menemukan file .MP4: dataset/training\\Sangat Tinggi\\Muhammad Rayhan_clip1.mp4\n",
      "Extraction complete for Muhammad Rayhan_clip1.mp4, saved 260 frames to dataset/training_images\\Sangat Tinggi\\Muhammad Rayhan_clip1\n",
      "Menemukan file .MP4: dataset/training\\Sangat Tinggi\\Muhammad Rayhan_clip2.mp4\n",
      "Extraction complete for Muhammad Rayhan_clip2.mp4, saved 260 frames to dataset/training_images\\Sangat Tinggi\\Muhammad Rayhan_clip2\n",
      "Menemukan file .MP4: dataset/training\\Sangat Tinggi\\Putra Zakaria Muzaki_clip1.mp4\n",
      "Extraction complete for Putra Zakaria Muzaki_clip1.mp4, saved 260 frames to dataset/training_images\\Sangat Tinggi\\Putra Zakaria Muzaki_clip1\n",
      "Menemukan file .MP4: dataset/training\\Sangat Tinggi\\Putra Zakaria Muzaki_clip2.mp4\n",
      "Extraction complete for Putra Zakaria Muzaki_clip2.mp4, saved 260 frames to dataset/training_images\\Sangat Tinggi\\Putra Zakaria Muzaki_clip2\n",
      "Menemukan file .MP4: dataset/training\\Sangat Tinggi\\Rayyan Al Firdausi_clip1.mp4\n",
      "Extraction complete for Rayyan Al Firdausi_clip1.mp4, saved 260 frames to dataset/training_images\\Sangat Tinggi\\Rayyan Al Firdausi_clip1\n",
      "Menemukan file .MP4: dataset/training\\Sangat Tinggi\\Rayyan Al Firdausi_clip2.mp4\n",
      "Extraction complete for Rayyan Al Firdausi_clip2.mp4, saved 260 frames to dataset/training_images\\Sangat Tinggi\\Rayyan Al Firdausi_clip2\n",
      "Memeriksa folder1: dataset/training\\Tinggi\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Abid Gimnastyar Alfiansyah_clip1.mp4\n",
      "Extraction complete for Abid Gimnastyar Alfiansyah_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Abid Gimnastyar Alfiansyah_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Abid Gimnastyar Alfiansyah_clip2.mp4\n",
      "Extraction complete for Abid Gimnastyar Alfiansyah_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Abid Gimnastyar Alfiansyah_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Ahmad Fathir Syafaat_clip1.mp4\n",
      "Extraction complete for Ahmad Fathir Syafaat_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Ahmad Fathir Syafaat_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Ahmad Fathir Syafaat_clip2.mp4\n",
      "Extraction complete for Ahmad Fathir Syafaat_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Ahmad Fathir Syafaat_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Akhmad Khoirul Falah_clip1.mp4\n",
      "Extraction complete for Akhmad Khoirul Falah_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Akhmad Khoirul Falah_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Akhmad Khoirul Falah_clip2.mp4\n",
      "Extraction complete for Akhmad Khoirul Falah_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Akhmad Khoirul Falah_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Aulia Adha A I_clip1.mp4\n",
      "Extraction complete for Aulia Adha A I_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Aulia Adha A I_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Aulia Adha A I_clip2.mp4\n",
      "Extraction complete for Aulia Adha A I_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Aulia Adha A I_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Brilyan Satria Wahyuda_clip1.mp4\n",
      "Extraction complete for Brilyan Satria Wahyuda_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Brilyan Satria Wahyuda_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Brilyan Satria Wahyuda_clip2.mp4\n",
      "Extraction complete for Brilyan Satria Wahyuda_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Brilyan Satria Wahyuda_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Broto Agung W_clip1.mp4\n",
      "Extraction complete for Broto Agung W_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Broto Agung W_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Broto Agung W_clip2.mp4\n",
      "Extraction complete for Broto Agung W_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Broto Agung W_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Denny Malik Ibrahim_clip1.mp4\n",
      "Extraction complete for Denny Malik Ibrahim_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Denny Malik Ibrahim_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Denny Malik Ibrahim_clip2.mp4\n",
      "Extraction complete for Denny Malik Ibrahim_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Denny Malik Ibrahim_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Doni Wahyu Kurniawan_clip1.mp4\n",
      "Extraction complete for Doni Wahyu Kurniawan_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Doni Wahyu Kurniawan_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Doni Wahyu Kurniawan_clip2.mp4\n",
      "Extraction complete for Doni Wahyu Kurniawan_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Doni Wahyu Kurniawan_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Fakridana Ahmad Rayyansyah_clip1.mp4\n",
      "Extraction complete for Fakridana Ahmad Rayyansyah_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Fakridana Ahmad Rayyansyah_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Fakridana Ahmad Rayyansyah_clip2.mp4\n",
      "Extraction complete for Fakridana Ahmad Rayyansyah_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Fakridana Ahmad Rayyansyah_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Farid Fitriansah Alfarizi_clip1.mp4\n",
      "Extraction complete for Farid Fitriansah Alfarizi_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Farid Fitriansah Alfarizi_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Farid Fitriansah Alfarizi_clip2.mp4\n",
      "Extraction complete for Farid Fitriansah Alfarizi_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Farid Fitriansah Alfarizi_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Fatriya Ibnu Ash Shidiqqi_clip1.mp4\n",
      "Extraction complete for Fatriya Ibnu Ash Shidiqqi_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Fatriya Ibnu Ash Shidiqqi_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Fatriya Ibnu Ash Shidiqqi_clip2.mp4\n",
      "Extraction complete for Fatriya Ibnu Ash Shidiqqi_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Fatriya Ibnu Ash Shidiqqi_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Halur Muhammad Abiyyu_clip1.mp4\n",
      "Extraction complete for Halur Muhammad Abiyyu_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Halur Muhammad Abiyyu_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Halur Muhammad Abiyyu_clip2.mp4\n",
      "Extraction complete for Halur Muhammad Abiyyu_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Halur Muhammad Abiyyu_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Juan Felix Antonio_clip1.mp4\n",
      "Extraction complete for Juan Felix Antonio_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Juan Felix Antonio_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Juan Felix Antonio_clip2.mp4\n",
      "Extraction complete for Juan Felix Antonio_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Juan Felix Antonio_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Kinata Dewa Ariandi_clip1.mp4\n",
      "Extraction complete for Kinata Dewa Ariandi_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Kinata Dewa Ariandi_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Kinata Dewa Ariandi_clip2.mp4\n",
      "Extraction complete for Kinata Dewa Ariandi_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Kinata Dewa Ariandi_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\M Bagus Indrawan_clip1.mp4\n",
      "Extraction complete for M Bagus Indrawan_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\M Bagus Indrawan_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\M Bagus Indrawan_clip2.mp4\n",
      "Extraction complete for M Bagus Indrawan_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\M Bagus Indrawan_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\M Iqbal Makmur A_clip1.mp4\n",
      "Extraction complete for M Iqbal Makmur A_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\M Iqbal Makmur A_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\M Iqbal Makmur A_clip2.mp4\n",
      "Extraction complete for M Iqbal Makmur A_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\M Iqbal Makmur A_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\M Isyad Dany_clip1.mp4\n",
      "Extraction complete for M Isyad Dany_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\M Isyad Dany_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\M Isyad Dany_clip2.mp4\n",
      "Extraction complete for M Isyad Dany_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\M Isyad Dany_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\M Naufal Syahendra_clip1.mp4\n",
      "Extraction complete for M Naufal Syahendra_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\M Naufal Syahendra_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\M Naufal Syahendra_clip2.mp4\n",
      "Extraction complete for M Naufal Syahendra_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\M Naufal Syahendra_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\M Tryo Bagus Anugrah P_clip1.mp4\n",
      "Extraction complete for M Tryo Bagus Anugrah P_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\M Tryo Bagus Anugrah P_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\M Tryo Bagus Anugrah P_clip2.mp4\n",
      "Extraction complete for M Tryo Bagus Anugrah P_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\M Tryo Bagus Anugrah P_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Maulidin Zakaria_clip1.mp4\n",
      "Extraction complete for Maulidin Zakaria_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Maulidin Zakaria_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Maulidin Zakaria_clip2.mp4\n",
      "Extraction complete for Maulidin Zakaria_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Maulidin Zakaria_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Pascalis Dewangga Sandi_clip1.mp4\n",
      "Extraction complete for Pascalis Dewangga Sandi_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Pascalis Dewangga Sandi_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Pascalis Dewangga Sandi_clip2.mp4\n",
      "Extraction complete for Pascalis Dewangga Sandi_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Pascalis Dewangga Sandi_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Ridho Fauzian Pratama_clip1.mp4\n",
      "Extraction complete for Ridho Fauzian Pratama_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Ridho Fauzian Pratama_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Ridho Fauzian Pratama_clip2.mp4\n",
      "Extraction complete for Ridho Fauzian Pratama_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Ridho Fauzian Pratama_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Rifki Setiawan_clip1.mp4\n",
      "Extraction complete for Rifki Setiawan_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Rifki Setiawan_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Rifki Setiawan_clip2.mp4\n",
      "Extraction complete for Rifki Setiawan_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Rifki Setiawan_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Satria Abrar Sambarana_clip1.mp4\n",
      "Extraction complete for Satria Abrar Sambarana_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Satria Abrar Sambarana_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Satria Abrar Sambarana_clip2.mp4\n",
      "Extraction complete for Satria Abrar Sambarana_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Satria Abrar Sambarana_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Stefanus Ageng Budi Utomo_clip1.mp4\n",
      "Extraction complete for Stefanus Ageng Budi Utomo_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Stefanus Ageng Budi Utomo_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Stefanus Ageng Budi Utomo_clip2.mp4\n",
      "Extraction complete for Stefanus Ageng Budi Utomo_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Stefanus Ageng Budi Utomo_clip2\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Yuma Rakha Samodra S_clip1.mp4\n",
      "Extraction complete for Yuma Rakha Samodra S_clip1.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Yuma Rakha Samodra S_clip1\n",
      "Menemukan file .MP4: dataset/training\\Tinggi\\Yuma Rakha Samodra S_clip2.mp4\n",
      "Extraction complete for Yuma Rakha Samodra S_clip2.mp4, saved 260 frames to dataset/training_images\\Tinggi\\Yuma Rakha Samodra S_clip2\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import shutil\n",
    "\n",
    "# # Path untuk video dan output\n",
    "# video_path = 'dataset/video/05_EP03_06.avi'\n",
    "# output_folder = 'dataset/casme_baru/Suprise/05_EP03_06'\n",
    "\n",
    "# # Buat folder output jika belum ada\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# # Buka video menggunakan OpenCV\n",
    "# vidcap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# # Ambil frame rate dari video\n",
    "# framePerSecond = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# # Variabel untuk menghitung nomor frame\n",
    "# count = 1\n",
    "\n",
    "# while vidcap.isOpened():\n",
    "#     success, image = vidcap.read()  # Baca frame\n",
    "#     if not success:\n",
    "#         break  # Jika tidak ada frame yang dibaca, keluar dari loop\n",
    "\n",
    "#     # Simpan gambar ke folder output dengan nama imgX.jpg\n",
    "#     img_name = f'img{count}.jpg'\n",
    "#     img_path = os.path.join(output_folder, img_name)\n",
    "#     cv2.imwrite(img_path, image)  # Simpan gambar\n",
    "\n",
    "#     # print(f'Saved frame {count} to {img_path}')\n",
    "\n",
    "#     count += 1\n",
    "\n",
    "# # Lepas video setelah selesai\n",
    "# vidcap.release()\n",
    "# print(f\"Extraction complete, saved {count-1} frames to {output_folder}\")\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "# Path utama\n",
    "base_input_folder = 'dataset/training'\n",
    "base_output_folder = 'dataset/training_images'\n",
    "\n",
    "# Bersihkan folder output jika ada\n",
    "if os.path.exists(base_output_folder):\n",
    "    shutil.rmtree(base_output_folder)\n",
    "\n",
    "# Loop melalui folder dan file dalam folder input\n",
    "for folder1 in os.listdir(base_input_folder):\n",
    "    folder1_path = os.path.join(base_input_folder, folder1)\n",
    "    \n",
    "    print(f\"Memeriksa folder1: {folder1_path}\")\n",
    "    \n",
    "    if os.path.isdir(folder1_path):\n",
    "        for filename in os.listdir(folder1_path):\n",
    "            file_path = os.path.join(folder1_path, filename)\n",
    "            \n",
    "            # Cek apakah file adalah file .avi\n",
    "            if filename.lower().endswith('.mp4') and os.path.isfile(file_path):\n",
    "                print(f\"Menemukan file .MP4: {file_path}\")\n",
    "\n",
    "                # Buat folder output sesuai nama folder\n",
    "                output_folder = os.path.join(base_output_folder, folder1, filename[:-4])  # Hapus ekstensi '.avi' dari nama folder\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "                # Buka video menggunakan OpenCV\n",
    "                vidcap = cv2.VideoCapture(file_path)\n",
    "\n",
    "                # Ambil frame rate dari video\n",
    "                framePerSecond = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "                # Variabel untuk menghitung nomor frame\n",
    "                count = 1\n",
    "\n",
    "                while vidcap.isOpened():\n",
    "                    success, image = vidcap.read()  # Baca frame\n",
    "                    if not success:\n",
    "                        break  # Jika tidak ada frame yang dibaca, keluar dari loop\n",
    "\n",
    "                    # Simpan gambar ke folder output dengan nama imgX.jpg\n",
    "                    img_name = f'img{count}.jpg'\n",
    "                    img_path = os.path.join(output_folder, img_name)\n",
    "                    cv2.imwrite(img_path, image)  # Simpan gambar\n",
    "\n",
    "                    count += 1\n",
    "\n",
    "                # Lepas video setelah selesai\n",
    "                vidcap.release()\n",
    "                print(f\"Extraction complete for {filename}, saved {count-1} frames to {output_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current folder: d:\\skripsi-ekspresi-mikro\\services\\ml-pipeline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Lihat current working directory\n",
    "print(\"Current folder:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolom tersedia: ['Frame', 'Folder Path', 'Label', 'mulut-X1', 'mulut-Y1', 'mulut-Tetha1', 'mulut-Magnitude1', 'mulut-X2', 'mulut-Y2', 'mulut-Tetha2', 'mulut-Magnitude2', 'mulut-X3', 'mulut-Y3', 'mulut-Tetha3', 'mulut-Magnitude3', 'mulut-X4', 'mulut-Y4', 'mulut-Tetha4', 'mulut-Magnitude4', 'mulut-X5', 'mulut-Y5', 'mulut-Tetha5', 'mulut-Magnitude5', 'mulut-X6', 'mulut-Y6', 'mulut-Tetha6', 'mulut-Magnitude6', 'mulut-X7', 'mulut-Y7', 'mulut-Tetha7', 'mulut-Magnitude7', 'mulut-X8', 'mulut-Y8', 'mulut-Tetha8', 'mulut-Magnitude8', 'mulut-X9', 'mulut-Y9', 'mulut-Tetha9', 'mulut-Magnitude9', 'mulut-X10', 'mulut-Y10', 'mulut-Tetha10', 'mulut-Magnitude10', 'mulut-X11', 'mulut-Y11', 'mulut-Tetha11', 'mulut-Magnitude11', 'mulut-X12', 'mulut-Y12', 'mulut-Tetha12', 'mulut-Magnitude12', 'mulut-X13', 'mulut-Y13', 'mulut-Tetha13', 'mulut-Magnitude13', 'mulut-X14', 'mulut-Y14', 'mulut-Tetha14', 'mulut-Magnitude14', 'mulut-X15', 'mulut-Y15', 'mulut-Tetha15', 'mulut-Magnitude15', 'mulut-X16', 'mulut-Y16', 'mulut-Tetha16', 'mulut-Magnitude16', 'mulut-X17', 'mulut-Y17', 'mulut-Tetha17', 'mulut-Magnitude17', 'mulut-X18', 'mulut-Y18', 'mulut-Tetha18', 'mulut-Magnitude18', 'mulut-X19', 'mulut-Y19', 'mulut-Tetha19', 'mulut-Magnitude19', 'mulut-X20', 'mulut-Y20', 'mulut-Tetha20', 'mulut-Magnitude20', 'mulut-X21', 'mulut-Y21', 'mulut-Tetha21', 'mulut-Magnitude21', 'mulut-X22', 'mulut-Y22', 'mulut-Tetha22', 'mulut-Magnitude22', 'mulut-X23', 'mulut-Y23', 'mulut-Tetha23', 'mulut-Magnitude23', 'mulut-X24', 'mulut-Y24', 'mulut-Tetha24', 'mulut-Magnitude24', 'mulut-X25', 'mulut-Y25', 'mulut-Tetha25', 'mulut-Magnitude25', 'mulut-X26', 'mulut-Y26', 'mulut-Tetha26', 'mulut-Magnitude26', 'mulut-X27', 'mulut-Y27', 'mulut-Tetha27', 'mulut-Magnitude27', 'mulut-X28', 'mulut-Y28', 'mulut-Tetha28', 'mulut-Magnitude28', 'mulut-X29', 'mulut-Y29', 'mulut-Tetha29', 'mulut-Magnitude29', 'mulut-X30', 'mulut-Y30', 'mulut-Tetha30', 'mulut-Magnitude30', 'mulut-X31', 'mulut-Y31', 'mulut-Tetha31', 'mulut-Magnitude31', 'mulut-X32', 'mulut-Y32', 'mulut-Tetha32', 'mulut-Magnitude32', 'mulut-X33', 'mulut-Y33', 'mulut-Tetha33', 'mulut-Magnitude33', 'mulut-X34', 'mulut-Y34', 'mulut-Tetha34', 'mulut-Magnitude34', 'mulut-X35', 'mulut-Y35', 'mulut-Tetha35', 'mulut-Magnitude35', 'mulut-X36', 'mulut-Y36', 'mulut-Tetha36', 'mulut-Magnitude36', 'mulut-X37', 'mulut-Y37', 'mulut-Tetha37', 'mulut-Magnitude37', 'mulut-X38', 'mulut-Y38', 'mulut-Tetha38', 'mulut-Magnitude38', 'mulut-X39', 'mulut-Y39', 'mulut-Tetha39', 'mulut-Magnitude39', 'mulut-X40', 'mulut-Y40', 'mulut-Tetha40', 'mulut-Magnitude40', 'mulut-X41', 'mulut-Y41', 'mulut-Tetha41', 'mulut-Magnitude41', 'mulut-X42', 'mulut-Y42', 'mulut-Tetha42', 'mulut-Magnitude42', 'mulut-X43', 'mulut-Y43', 'mulut-Tetha43', 'mulut-Magnitude43', 'mulut-X44', 'mulut-Y44', 'mulut-Tetha44', 'mulut-Magnitude44', 'mulut-X45', 'mulut-Y45', 'mulut-Tetha45', 'mulut-Magnitude45', 'mulut-X46', 'mulut-Y46', 'mulut-Tetha46', 'mulut-Magnitude46', 'mulut-X47', 'mulut-Y47', 'mulut-Tetha47', 'mulut-Magnitude47', 'mulut-X48', 'mulut-Y48', 'mulut-Tetha48', 'mulut-Magnitude48', 'mulut-X49', 'mulut-Y49', 'mulut-Tetha49', 'mulut-Magnitude49', 'mulut-X50', 'mulut-Y50', 'mulut-Tetha50', 'mulut-Magnitude50', 'mulut-X51', 'mulut-Y51', 'mulut-Tetha51', 'mulut-Magnitude51', 'mulut-X52', 'mulut-Y52', 'mulut-Tetha52', 'mulut-Magnitude52', 'mulut-X53', 'mulut-Y53', 'mulut-Tetha53', 'mulut-Magnitude53', 'mulut-X54', 'mulut-Y54', 'mulut-Tetha54', 'mulut-Magnitude54', 'mulut-X55', 'mulut-Y55', 'mulut-Tetha55', 'mulut-Magnitude55', 'mulut-X56', 'mulut-Y56', 'mulut-Tetha56', 'mulut-Magnitude56', 'mulut-X57', 'mulut-Y57', 'mulut-Tetha57', 'mulut-Magnitude57', 'mulut-X58', 'mulut-Y58', 'mulut-Tetha58', 'mulut-Magnitude58', 'mulut-X59', 'mulut-Y59', 'mulut-Tetha59', 'mulut-Magnitude59', 'mulut-X60', 'mulut-Y60', 'mulut-Tetha60', 'mulut-Magnitude60', 'mulut-X61', 'mulut-Y61', 'mulut-Tetha61', 'mulut-Magnitude61', 'mulut-X62', 'mulut-Y62', 'mulut-Tetha62', 'mulut-Magnitude62', 'mulut-X63', 'mulut-Y63', 'mulut-Tetha63', 'mulut-Magnitude63', 'mulut-X64', 'mulut-Y64', 'mulut-Tetha64', 'mulut-Magnitude64', 'mulut-X65', 'mulut-Y65', 'mulut-Tetha65', 'mulut-Magnitude65', 'mulut-X66', 'mulut-Y66', 'mulut-Tetha66', 'mulut-Magnitude66', 'mulut-X67', 'mulut-Y67', 'mulut-Tetha67', 'mulut-Magnitude67', 'mulut-X68', 'mulut-Y68', 'mulut-Tetha68', 'mulut-Magnitude68', 'mulut-X69', 'mulut-Y69', 'mulut-Tetha69', 'mulut-Magnitude69', 'mulut-X70', 'mulut-Y70', 'mulut-Tetha70', 'mulut-Magnitude70', 'mulut-X71', 'mulut-Y71', 'mulut-Tetha71', 'mulut-Magnitude71', 'mulut-X72', 'mulut-Y72', 'mulut-Tetha72', 'mulut-Magnitude72', 'mulut-X73', 'mulut-Y73', 'mulut-Tetha73', 'mulut-Magnitude73', 'mulut-X74', 'mulut-Y74', 'mulut-Tetha74', 'mulut-Magnitude74', 'mulut-X75', 'mulut-Y75', 'mulut-Tetha75', 'mulut-Magnitude75', 'mulut-X76', 'mulut-Y76', 'mulut-Tetha76', 'mulut-Magnitude76', 'mulut-X77', 'mulut-Y77', 'mulut-Tetha77', 'mulut-Magnitude77', 'mulut-X78', 'mulut-Y78', 'mulut-Tetha78', 'mulut-Magnitude78', 'mulut-X79', 'mulut-Y79', 'mulut-Tetha79', 'mulut-Magnitude79', 'mulut-X80', 'mulut-Y80', 'mulut-Tetha80', 'mulut-Magnitude80', 'mulut-X81', 'mulut-Y81', 'mulut-Tetha81', 'mulut-Magnitude81', 'mulut-X82', 'mulut-Y82', 'mulut-Tetha82', 'mulut-Magnitude82', 'mulut-X83', 'mulut-Y83', 'mulut-Tetha83', 'mulut-Magnitude83', 'mulut-X84', 'mulut-Y84', 'mulut-Tetha84', 'mulut-Magnitude84', 'mulut-X85', 'mulut-Y85', 'mulut-Tetha85', 'mulut-Magnitude85', 'mulut-X86', 'mulut-Y86', 'mulut-Tetha86', 'mulut-Magnitude86', 'mulut-X87', 'mulut-Y87', 'mulut-Tetha87', 'mulut-Magnitude87', 'mulut-X88', 'mulut-Y88', 'mulut-Tetha88', 'mulut-Magnitude88', 'mulut-X89', 'mulut-Y89', 'mulut-Tetha89', 'mulut-Magnitude89', 'mulut-X90', 'mulut-Y90', 'mulut-Tetha90', 'mulut-Magnitude90', 'mulut-X91', 'mulut-Y91', 'mulut-Tetha91', 'mulut-Magnitude91', 'mulut-X92', 'mulut-Y92', 'mulut-Tetha92', 'mulut-Magnitude92', 'mulut-X93', 'mulut-Y93', 'mulut-Tetha93', 'mulut-Magnitude93', 'mulut-X94', 'mulut-Y94', 'mulut-Tetha94', 'mulut-Magnitude94', 'mulut-X95', 'mulut-Y95', 'mulut-Tetha95', 'mulut-Magnitude95', 'mulut-X96', 'mulut-Y96', 'mulut-Tetha96', 'mulut-Magnitude96', 'mulut-X97', 'mulut-Y97', 'mulut-Tetha97', 'mulut-Magnitude97', 'mulut-X98', 'mulut-Y98', 'mulut-Tetha98', 'mulut-Magnitude98', 'mulut-X99', 'mulut-Y99', 'mulut-Tetha99', 'mulut-Magnitude99', 'mulut-X100', 'mulut-Y100', 'mulut-Tetha100', 'mulut-Magnitude100', 'alis-X1', 'alis-Y1', 'alis-Tetha1', 'alis-Magnitude1', 'alis-X2', 'alis-Y2', 'alis-Tetha2', 'alis-Magnitude2', 'alis-X3', 'alis-Y3', 'alis-Tetha3', 'alis-Magnitude3', 'alis-X4', 'alis-Y4', 'alis-Tetha4', 'alis-Magnitude4', 'alis-X5', 'alis-Y5', 'alis-Tetha5', 'alis-Magnitude5', 'alis-X6', 'alis-Y6', 'alis-Tetha6', 'alis-Magnitude6', 'alis-X7', 'alis-Y7', 'alis-Tetha7', 'alis-Magnitude7', 'alis-X8', 'alis-Y8', 'alis-Tetha8', 'alis-Magnitude8', 'alis-X9', 'alis-Y9', 'alis-Tetha9', 'alis-Magnitude9', 'alis-X10', 'alis-Y10', 'alis-Tetha10', 'alis-Magnitude10', 'alis-X11', 'alis-Y11', 'alis-Tetha11', 'alis-Magnitude11', 'alis-X12', 'alis-Y12', 'alis-Tetha12', 'alis-Magnitude12', 'alis-X13', 'alis-Y13', 'alis-Tetha13', 'alis-Magnitude13', 'alis-X14', 'alis-Y14', 'alis-Tetha14', 'alis-Magnitude14', 'alis-X15', 'alis-Y15', 'alis-Tetha15', 'alis-Magnitude15', 'alis-X16', 'alis-Y16', 'alis-Tetha16', 'alis-Magnitude16', 'alis-X17', 'alis-Y17', 'alis-Tetha17', 'alis-Magnitude17', 'alis-X18', 'alis-Y18', 'alis-Tetha18', 'alis-Magnitude18', 'alis-X19', 'alis-Y19', 'alis-Tetha19', 'alis-Magnitude19', 'alis-X20', 'alis-Y20', 'alis-Tetha20', 'alis-Magnitude20', 'alis-X21', 'alis-Y21', 'alis-Tetha21', 'alis-Magnitude21', 'alis-X22', 'alis-Y22', 'alis-Tetha22', 'alis-Magnitude22', 'alis-X23', 'alis-Y23', 'alis-Tetha23', 'alis-Magnitude23', 'alis-X24', 'alis-Y24', 'alis-Tetha24', 'alis-Magnitude24', 'alis-X25', 'alis-Y25', 'alis-Tetha25', 'alis-Magnitude25', 'alis-X26', 'alis-Y26', 'alis-Tetha26', 'alis-Magnitude26', 'alis-X27', 'alis-Y27', 'alis-Tetha27', 'alis-Magnitude27', 'alis-X28', 'alis-Y28', 'alis-Tetha28', 'alis-Magnitude28', 'alis-X29', 'alis-Y29', 'alis-Tetha29', 'alis-Magnitude29', 'alis-X30', 'alis-Y30', 'alis-Tetha30', 'alis-Magnitude30', 'alis-X31', 'alis-Y31', 'alis-Tetha31', 'alis-Magnitude31', 'alis-X32', 'alis-Y32', 'alis-Tetha32', 'alis-Magnitude32', 'alis-X33', 'alis-Y33', 'alis-Tetha33', 'alis-Magnitude33', 'alis-X34', 'alis-Y34', 'alis-Tetha34', 'alis-Magnitude34', 'alis-X35', 'alis-Y35', 'alis-Tetha35', 'alis-Magnitude35', 'alis-X36', 'alis-Y36', 'alis-Tetha36', 'alis-Magnitude36', 'alis-X37', 'alis-Y37', 'alis-Tetha37', 'alis-Magnitude37', 'alis-X38', 'alis-Y38', 'alis-Tetha38', 'alis-Magnitude38', 'alis-X39', 'alis-Y39', 'alis-Tetha39', 'alis-Magnitude39', 'alis-X40', 'alis-Y40', 'alis-Tetha40', 'alis-Magnitude40', 'alis-X41', 'alis-Y41', 'alis-Tetha41', 'alis-Magnitude41', 'alis-X42', 'alis-Y42', 'alis-Tetha42', 'alis-Magnitude42', 'alis-X43', 'alis-Y43', 'alis-Tetha43', 'alis-Magnitude43', 'alis-X44', 'alis-Y44', 'alis-Tetha44', 'alis-Magnitude44', 'alis-X45', 'alis-Y45', 'alis-Tetha45', 'alis-Magnitude45', 'alis-X46', 'alis-Y46', 'alis-Tetha46', 'alis-Magnitude46', 'alis-X47', 'alis-Y47', 'alis-Tetha47', 'alis-Magnitude47', 'alis-X48', 'alis-Y48', 'alis-Tetha48', 'alis-Magnitude48', 'alis-X49', 'alis-Y49', 'alis-Tetha49', 'alis-Magnitude49', 'alis-X50', 'alis-Y50', 'alis-Tetha50', 'alis-Magnitude50', 'alis-X51', 'alis-Y51', 'alis-Tetha51', 'alis-Magnitude51', 'alis-X52', 'alis-Y52', 'alis-Tetha52', 'alis-Magnitude52', 'alis-X53', 'alis-Y53', 'alis-Tetha53', 'alis-Magnitude53', 'alis-X54', 'alis-Y54', 'alis-Tetha54', 'alis-Magnitude54', 'alis-X55', 'alis-Y55', 'alis-Tetha55', 'alis-Magnitude55', 'alis-X56', 'alis-Y56', 'alis-Tetha56', 'alis-Magnitude56', 'alis-X57', 'alis-Y57', 'alis-Tetha57', 'alis-Magnitude57', 'alis-X58', 'alis-Y58', 'alis-Tetha58', 'alis-Magnitude58', 'alis-X59', 'alis-Y59', 'alis-Tetha59', 'alis-Magnitude59', 'alis-X60', 'alis-Y60', 'alis-Tetha60', 'alis-Magnitude60', 'alis-X61', 'alis-Y61', 'alis-Tetha61', 'alis-Magnitude61', 'alis-X62', 'alis-Y62', 'alis-Tetha62', 'alis-Magnitude62', 'alis-X63', 'alis-Y63', 'alis-Tetha63', 'alis-Magnitude63', 'alis-X64', 'alis-Y64', 'alis-Tetha64', 'alis-Magnitude64', 'alis-X65', 'alis-Y65', 'alis-Tetha65', 'alis-Magnitude65', 'alis-X66', 'alis-Y66', 'alis-Tetha66', 'alis-Magnitude66', 'alis-X67', 'alis-Y67', 'alis-Tetha67', 'alis-Magnitude67', 'alis-X68', 'alis-Y68', 'alis-Tetha68', 'alis-Magnitude68', 'alis-X69', 'alis-Y69', 'alis-Tetha69', 'alis-Magnitude69', 'alis-X70', 'alis-Y70', 'alis-Tetha70', 'alis-Magnitude70', 'alis-X71', 'alis-Y71', 'alis-Tetha71', 'alis-Magnitude71', 'alis-X72', 'alis-Y72', 'alis-Tetha72', 'alis-Magnitude72', 'alis-X73', 'alis-Y73', 'alis-Tetha73', 'alis-Magnitude73', 'alis-X74', 'alis-Y74', 'alis-Tetha74', 'alis-Magnitude74', 'alis-X75', 'alis-Y75', 'alis-Tetha75', 'alis-Magnitude75', 'alis-X76', 'alis-Y76', 'alis-Tetha76', 'alis-Magnitude76', 'alis-X77', 'alis-Y77', 'alis-Tetha77', 'alis-Magnitude77', 'alis-X78', 'alis-Y78', 'alis-Tetha78', 'alis-Magnitude78', 'alis-X79', 'alis-Y79', 'alis-Tetha79', 'alis-Magnitude79', 'alis-X80', 'alis-Y80', 'alis-Tetha80', 'alis-Magnitude80', 'alis-X81', 'alis-Y81', 'alis-Tetha81', 'alis-Magnitude81', 'alis-X82', 'alis-Y82', 'alis-Tetha82', 'alis-Magnitude82', 'alis-X83', 'alis-Y83', 'alis-Tetha83', 'alis-Magnitude83', 'alis-X84', 'alis-Y84', 'alis-Tetha84', 'alis-Magnitude84', 'alis-X85', 'alis-Y85', 'alis-Tetha85', 'alis-Magnitude85', 'alis-X86', 'alis-Y86', 'alis-Tetha86', 'alis-Magnitude86', 'alis-X87', 'alis-Y87', 'alis-Tetha87', 'alis-Magnitude87', 'alis-X88', 'alis-Y88', 'alis-Tetha88', 'alis-Magnitude88', 'alis-X89', 'alis-Y89', 'alis-Tetha89', 'alis-Magnitude89', 'alis-X90', 'alis-Y90', 'alis-Tetha90', 'alis-Magnitude90', 'alis-X91', 'alis-Y91', 'alis-Tetha91', 'alis-Magnitude91', 'alis-X92', 'alis-Y92', 'alis-Tetha92', 'alis-Magnitude92', 'alis-X93', 'alis-Y93', 'alis-Tetha93', 'alis-Magnitude93', 'alis-X94', 'alis-Y94', 'alis-Tetha94', 'alis-Magnitude94', 'alis-X95', 'alis-Y95', 'alis-Tetha95', 'alis-Magnitude95', 'alis-X96', 'alis-Y96', 'alis-Tetha96', 'alis-Magnitude96', 'alis-X97', 'alis-Y97', 'alis-Tetha97', 'alis-Magnitude97', 'alis-X98', 'alis-Y98', 'alis-Tetha98', 'alis-Magnitude98', 'alis-X99', 'alis-Y99', 'alis-Tetha99', 'alis-Magnitude99', 'alis-X100', 'alis-Y100', 'alis-Tetha100', 'alis-Magnitude100', 'alis-X101', 'alis-Y101', 'alis-Tetha101', 'alis-Magnitude101', 'alis-X102', 'alis-Y102', 'alis-Tetha102', 'alis-Magnitude102', 'alis-X103', 'alis-Y103', 'alis-Tetha103', 'alis-Magnitude103', 'alis-X104', 'alis-Y104', 'alis-Tetha104', 'alis-Magnitude104', 'alis-X105', 'alis-Y105', 'alis-Tetha105', 'alis-Magnitude105', 'alis-X106', 'alis-Y106', 'alis-Tetha106', 'alis-Magnitude106', 'alis-X107', 'alis-Y107', 'alis-Tetha107', 'alis-Magnitude107', 'alis-X108', 'alis-Y108', 'alis-Tetha108', 'alis-Magnitude108', 'alis-X109', 'alis-Y109', 'alis-Tetha109', 'alis-Magnitude109', 'alis-X110', 'alis-Y110', 'alis-Tetha110', 'alis-Magnitude110', 'alis-X111', 'alis-Y111', 'alis-Tetha111', 'alis-Magnitude111', 'alis-X112', 'alis-Y112', 'alis-Tetha112', 'alis-Magnitude112', 'alis-X113', 'alis-Y113', 'alis-Tetha113', 'alis-Magnitude113', 'alis-X114', 'alis-Y114', 'alis-Tetha114', 'alis-Magnitude114', 'alis-X115', 'alis-Y115', 'alis-Tetha115', 'alis-Magnitude115', 'alis-X116', 'alis-Y116', 'alis-Tetha116', 'alis-Magnitude116', 'alis-X117', 'alis-Y117', 'alis-Tetha117', 'alis-Magnitude117', 'alis-X118', 'alis-Y118', 'alis-Tetha118', 'alis-Magnitude118', 'alis-X119', 'alis-Y119', 'alis-Tetha119', 'alis-Magnitude119', 'alis-X120', 'alis-Y120', 'alis-Tetha120', 'alis-Magnitude120', 'alis-X121', 'alis-Y121', 'alis-Tetha121', 'alis-Magnitude121', 'alis-X122', 'alis-Y122', 'alis-Tetha122', 'alis-Magnitude122', 'alis-X123', 'alis-Y123', 'alis-Tetha123', 'alis-Magnitude123', 'alis-X124', 'alis-Y124', 'alis-Tetha124', 'alis-Magnitude124', 'alis-X125', 'alis-Y125', 'alis-Tetha125', 'alis-Magnitude125', 'alis-X126', 'alis-Y126', 'alis-Tetha126', 'alis-Magnitude126', 'alis-X127', 'alis-Y127', 'alis-Tetha127', 'alis-Magnitude127', 'alis-X128', 'alis-Y128', 'alis-Tetha128', 'alis-Magnitude128', 'alis-X129', 'alis-Y129', 'alis-Tetha129', 'alis-Magnitude129', 'alis-X130', 'alis-Y130', 'alis-Tetha130', 'alis-Magnitude130', 'alis-X131', 'alis-Y131', 'alis-Tetha131', 'alis-Magnitude131', 'alis-X132', 'alis-Y132', 'alis-Tetha132', 'alis-Magnitude132', 'alis-X133', 'alis-Y133', 'alis-Tetha133', 'alis-Magnitude133', 'alis-X134', 'alis-Y134', 'alis-Tetha134', 'alis-Magnitude134', 'alis-X135', 'alis-Y135', 'alis-Tetha135', 'alis-Magnitude135', 'alis-X136', 'alis-Y136', 'alis-Tetha136', 'alis-Magnitude136', 'alis-X137', 'alis-Y137', 'alis-Tetha137', 'alis-Magnitude137', 'alis-X138', 'alis-Y138', 'alis-Tetha138', 'alis-Magnitude138', 'alis-X139', 'alis-Y139', 'alis-Tetha139', 'alis-Magnitude139', 'alis-X140', 'alis-Y140', 'alis-Tetha140', 'alis-Magnitude140']\n",
      "     Frame       Folder Path   Label  mulut-X1  mulut-Y1  mulut-Tetha1  \\\n",
      "0  1(img1)  Abdul Aziz_clip1  Rendah       NaN       NaN           NaN   \n",
      "1  2(img2)  Abdul Aziz_clip1  Rendah       0.0       0.0           0.0   \n",
      "2  3(img3)  Abdul Aziz_clip1  Rendah       0.0       0.0           0.0   \n",
      "3  4(img4)  Abdul Aziz_clip1  Rendah       0.0       0.0           0.0   \n",
      "4  5(img5)  Abdul Aziz_clip1  Rendah       0.0       0.0           0.0   \n",
      "\n",
      "   mulut-Magnitude1  mulut-X2  mulut-Y2  mulut-Tetha2  ...  alis-Tetha138  \\\n",
      "0               NaN       NaN       NaN           NaN  ...            NaN   \n",
      "1               0.0       0.0       0.0           0.0  ...        225.000   \n",
      "2               0.0       0.0       0.0           0.0  ...        213.690   \n",
      "3               0.0       0.0       0.0           0.0  ...         90.000   \n",
      "4               0.0       0.0       0.0           0.0  ...         18.435   \n",
      "\n",
      "   alis-Magnitude138  alis-X139  alis-Y139  alis-Tetha139  alis-Magnitude139  \\\n",
      "0                NaN        NaN        NaN            NaN                NaN   \n",
      "1              1.414        0.0        1.0           90.0                1.0   \n",
      "2              3.606        0.0        0.0            0.0                0.0   \n",
      "3              2.000        0.0       -3.0          270.0                3.0   \n",
      "4              3.162        0.0        0.0            0.0                0.0   \n",
      "\n",
      "   alis-X140  alis-Y140  alis-Tetha140  alis-Magnitude140  \n",
      "0        NaN        NaN            NaN                NaN  \n",
      "1        0.0        1.0           90.0              1.000  \n",
      "2        0.0        0.0            0.0              0.000  \n",
      "3       -1.0        1.0          135.0              1.414  \n",
      "4        0.0        0.0            0.0              0.000  \n",
      "\n",
      "[5 rows x 963 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Baca file CSV dari folder relatif\n",
    "df = pd.read_csv('output-baru/csv/nilai-fitur-all-component.csv')\n",
    "\n",
    "# Tampilkan kolom dan data awal\n",
    "print(\"Kolom tersedia:\", df.columns.tolist())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total kolom: 963\n",
      "Jumlah fitur: 960\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('output-baru/csv/nilai-fitur-all-component.csv')\n",
    "\n",
    "total_kolom = df.shape[1]\n",
    "\n",
    "# Hitung jumlah fitur (dengan asumsi 3 kolom pertama bukan fitur)\n",
    "jumlah_fitur = total_kolom - 3\n",
    "\n",
    "print(\"Total kolom:\", total_kolom)\n",
    "print(\"Jumlah fitur:\", jumlah_fitur)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import shutil\n",
    "np.set_printoptions(threshold=np.inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 130\u001b[0m\n\u001b[0;32m    128\u001b[0m initPOC \u001b[38;5;241m=\u001b[39m POC(data_blocks_first_image[component_name], data_blocks_image_current, blockSize) \n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Pemanggilan fungsi pocCalc() untuk menghitung nilai POC disetiap gambar\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m valPOC \u001b[38;5;241m=\u001b[39m \u001b[43minitPOC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetPOC\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Pemanggilan class dan method untuk menampilkan quiver / gambar panah\u001b[39;00m\n\u001b[0;32m    133\u001b[0m initQuiv \u001b[38;5;241m=\u001b[39m Vektor(valPOC, blockSize)\n",
      "File \u001b[1;32md:\\skripsi-ekspresi-mikro\\services\\ml-pipeline\\feature_extraction\\poc.py:94\u001b[0m, in \u001b[0;36mPOC.getPOC\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m block_curr \u001b[38;5;241m=\u001b[39m BlocksCurr[nY, nX]\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Perhitungan POC \u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalcPOC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_curr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmb_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmb_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# menyimpan nilai poc sesuai nomor blok\u001b[39;00m\n\u001b[0;32m     96\u001b[0m poc[:, :, nm] \u001b[38;5;241m=\u001b[39m r\n",
      "File \u001b[1;32md:\\skripsi-ekspresi-mikro\\services\\ml-pipeline\\feature_extraction\\poc.py:35\u001b[0m, in \u001b[0;36mPOC.calcPOC\u001b[1;34m(self, block_ref, block_curr, window, mb_x, mb_y)\u001b[0m\n\u001b[0;32m     33\u001b[0m r   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(r) \n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Menggeser hasil invers transformasi Fourier agar titik nol berada di tengah\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m r   \u001b[38;5;241m=\u001b[39m \u001b[43mfftshift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\Fina Orivia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\fft\\helper.py:73\u001b[0m, in \u001b[0;36mfftshift\u001b[1;34m(x, axes)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     shift \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mshape[ax] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m axes]\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mroll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Fina Orivia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\numeric.py:1231\u001b[0m, in \u001b[0;36mroll\u001b[1;34m(a, shift, axis)\u001b[0m\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m indices \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mproduct(\u001b[38;5;241m*\u001b[39mrolls):\n\u001b[0;32m   1230\u001b[0m     arr_index, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mindices)\n\u001b[1;32m-> 1231\u001b[0m     result[res_index] \u001b[38;5;241m=\u001b[39m a[arr_index]\n\u001b[0;32m   1233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import module yang digunakan\n",
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from preprocessing.scarpping_component import extract_component_by_images\n",
    "from feature_extraction.poc import POC\n",
    "from feature_extraction.vektor import Vektor\n",
    "from feature_extraction.quadran import Quadran\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('(\\d+)', s)]\n",
    "\n",
    "# load model dan shape predictor untuk deteksi wajah\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"models/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "components_setup = {\n",
    "    'mulut': {\n",
    "        'object_name': 'Mouth',\n",
    "        'object_rectangle': {\"x_right\": 54, \"x_left\": 48, \"y_highest\": 52, \"y_lowest\": 57},\n",
    "        'pixel_shifting': {\"pixel_x\": 25, \"pixel_y\": 10},\n",
    "        'object_dimension': {'width': 140, 'height': 40}\n",
    "    },\n",
    "    'alis': {\n",
    "        'object_name': 'Eyebrows',\n",
    "        'object_rectangle': {\"x_right\": 26, \"x_left\": 17, \"y_highest\": 18, \"y_lowest\": 25},\n",
    "        'pixel_shifting': {\"pixel_x\": 20, \"pixel_y\": 15},\n",
    "        'object_dimension': {'width': 200, 'height': 40}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Inisialisasi variabel untuk path url dan blok size\n",
    "blockSize = 7\n",
    "pathDirectory = {\n",
    "    \"base_path_dataset\": \"dataset\",\n",
    "    \"base_dataset_input\": \"dataset/training_images\",\n",
    "    \"result_dataset\": \"output-baru\", \n",
    "    \"component_image\": \"component_to_images\", \n",
    "    \"video_frame\": \"video_to_images\"\n",
    "}\n",
    "\n",
    "# Inisialisasi variabel untuk menyimpan data dari masing-masing komponen\n",
    "frames_data = {component_name: [] for component_name in components_setup}\n",
    "total_blocks_components = {component_name: 0 for component_name in components_setup}\n",
    "data_blocks_first_image = {component_name: None for component_name in components_setup}\n",
    "index = {component_name: 0 for component_name in components_setup}\n",
    "\n",
    "# Hitung total blok dari masing-masing komponen lalu disetup kedalam total_blocks_components\n",
    "for component_name, component_info in components_setup.items():\n",
    "    total_blocks_components[component_name] = int((component_info['object_dimension']['width'] / blockSize) * (component_info['object_dimension']['height'] / blockSize))\n",
    "\n",
    "# Loop melalui setiap kunci dan nilai di pathDirectory dan hapus direktorinya\n",
    "for key, value in pathDirectory.items():\n",
    "    # Jika tidak mengandung base, hapus direktori sesuai dengan nilai yang ada\n",
    "    if not \"base\" in key:\n",
    "        shutil.rmtree(value, ignore_errors=True)\n",
    "\n",
    "# Looping folder label didalam base_dataset_input\n",
    "for foldername_label in os.listdir(pathDirectory['base_dataset_input']):\n",
    "    foldername_label_join_basepath = os.path.join(pathDirectory['base_dataset_input'], foldername_label)\n",
    "    # Jika folder nya tidak ada skip looping ini\n",
    "    if not os.path.isdir(foldername_label_join_basepath):\n",
    "        continue\n",
    "    \n",
    "    # Looping folder label didalam foldername_label_join_basepath \n",
    "    for foldername in os.listdir(foldername_label_join_basepath):\n",
    "        foldername_join_basepath = os.path.join(foldername_label_join_basepath, foldername)\n",
    "        # Jika folder nya tidak ada skip looping ini\n",
    "        if not os.path.isdir(foldername_join_basepath):\n",
    "            continue\n",
    "\n",
    "        # Reset variabel setiap kali mulai looping folder baru\n",
    "        data_blocks_first_image = {component_name: None for component_name in components_setup}\n",
    "        index = {component_name: 0 for component_name in components_setup}\n",
    "\n",
    "        # looping semua file yang ada didalam\n",
    "        for filename in sorted(os.listdir(foldername_join_basepath), key=natural_sort_key):\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"): \n",
    "\n",
    "                # Read path sesuai dengan foldername_join_basepath dijoin path dengan filename\n",
    "                image = cv2.imread(os.path.join(foldername_join_basepath, filename))\n",
    "                # Convert image ke grayscale\n",
    "                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # Deteksi shape muka didalam grayscale image\n",
    "                rects = detector(gray)\n",
    "\n",
    "                # Memproses rects untuk setiap bentuk wajah yang terdeteksi\n",
    "                for rect in rects:\n",
    "                    # Ambil bentuk wajah dalam bentuk shape sesuai dengan model predictor\n",
    "                    shape = predictor(gray, rect)\n",
    "                    # Memproses setiap komponen wajah\n",
    "                    for component_name, component_info in components_setup.items():\n",
    "                        # Buat variabel frame_data untuk menampung data current frame\n",
    "                        frame_data = {'Frame': f\"{index[component_name] + 1}({filename.split('.')[0]})\"}\n",
    "\n",
    "                        # Ambil data blok image dari return fungsi extract_component_by_images\n",
    "                        data_blocks_image_current = extract_component_by_images(\n",
    "                            image=image,\n",
    "                            shape=shape,\n",
    "                            frameName=filename.split(\".\")[0],\n",
    "                            objectName=component_info['object_name'],\n",
    "                            objectRectangle=component_info['object_rectangle'],\n",
    "                            pixelShifting=component_info['pixel_shifting'],\n",
    "                            objectDimension=component_info['object_dimension']\n",
    "                        )\n",
    "                        \n",
    "                        # Ambil frame pertama dari perulangan lalu simpan di variabel dan skip (lanjutkan ke frame berikut)\n",
    "                        if data_blocks_first_image[component_name] is None:\n",
    "                            # Append data frame ke list frames_data sesuai dengan component_name\n",
    "                            frames_data[component_name].append(frame_data)\n",
    "                            # Tambahkan kolom \"Folder Path\" dengan nilai folder saat ini\n",
    "                            frame_data['Folder Path'] = foldername\n",
    "                            # Tambahkan kolom \"Label\" dengan nilai label saat ini\n",
    "                            frame_data['Label'] = foldername_label\n",
    "                            # Set value data_blocks_first_image[component_name] ke data_blocks_image_current\n",
    "                            data_blocks_first_image[component_name] = data_blocks_image_current\n",
    "                            # Skip looping nya ke looping selanjutnya\n",
    "                            continue\n",
    "\n",
    "                        # Inisiasi class POC\n",
    "                        initPOC = POC(data_blocks_first_image[component_name], data_blocks_image_current, blockSize) \n",
    "                        # Pemanggilan fungsi pocCalc() untuk menghitung nilai POC disetiap gambar\n",
    "                        valPOC = initPOC.getPOC() \n",
    "\n",
    "                        # Pemanggilan class dan method untuk menampilkan quiver / gambar panah\n",
    "                        initQuiv = Vektor(valPOC, blockSize)\n",
    "                        quivData = initQuiv.getVektor() \n",
    "\n",
    "                        # Pemanggilan class untuk mengeluarkan nilai karakteristik vektor\n",
    "                        initQuadran = Quadran(quivData) \n",
    "                        quadran = initQuadran.getQuadran()\n",
    "\n",
    "                        # Update frame_data dengan data quadran\n",
    "                        for i, quad in enumerate(quadran):\n",
    "                            # Set data kedalam frame_data sesuai column nya\n",
    "                            frame_data[f'X{i+1}'] = quad[1]\n",
    "                            frame_data[f'Y{i+1}'] = quad[2]\n",
    "                            frame_data[f'Tetha{i+1}'] = quad[3]\n",
    "                            frame_data[f'Magnitude{i+1}'] = quad[4]\n",
    "                        \n",
    "                        # Append data frame ke list\n",
    "                        frames_data[component_name].append(frame_data)\n",
    "                        # Tambahkan kolom \"Folder Path\" dengan nilai folder saat ini\n",
    "                        frame_data['Folder Path'] = foldername\n",
    "                        # Tambahkan kolom \"Label\" dengan nilai label saat ini\n",
    "                        frame_data['Label'] = foldername_label\n",
    "\n",
    "                # Update index per component_name\n",
    "                index[component_name] += 1\n",
    "\n",
    "# Membuat direktori jika belum ada untuk outputnya\n",
    "output_csv_dir = os.path.join(pathDirectory['result_dataset'], 'csv')\n",
    "output_excel_dir = os.path.join(pathDirectory['result_dataset'], 'excel')\n",
    "os.makedirs(output_csv_dir, exist_ok=True)\n",
    "os.makedirs(output_excel_dir, exist_ok=True)\n",
    "\n",
    "# Hapus file output dari semua tipe dataset baik csv dan xlsx jika ada (Nilai fitur)\n",
    "for component_name in components_setup:\n",
    "    csv_file_path = os.path.join(output_csv_dir, f'{component_name}.csv')\n",
    "    xlsx_file_path = os.path.join(output_excel_dir, f'{component_name}.xlsx')\n",
    "    if os.path.exists(csv_file_path):\n",
    "        os.remove(csv_file_path)\n",
    "    if os.path.exists(xlsx_file_path):\n",
    "        os.remove(xlsx_file_path)\n",
    "\n",
    "# Export dataframe ke dalam file csv dan xlsx sesuai dengan masing-masing component untuk fitur ekstrasi biasa\n",
    "for component_name, data_frame in frames_data.items():\n",
    "    # Initialisasi dataframe dengan pandas\n",
    "    df = pd.DataFrame(data_frame)\n",
    "    \n",
    "    # Simpan ke file CSV\n",
    "    nama_file_csv = f'{output_csv_dir}/{component_name}.csv'\n",
    "    df.to_csv(nama_file_csv, index=False, float_format=None)\n",
    "    \n",
    "    # Simpan ke file Excel\n",
    "    nama_file_xlsx = f'{output_excel_dir}/{component_name}.xlsx'\n",
    "    df.to_excel(nama_file_xlsx, index=False, float_format=None)\n",
    "\n",
    "# Export dataframe untuk semua komponen digabung\n",
    "frames_data_all_components = []\n",
    "\n",
    "# Gabungkan semua data frame dari semua komponen\n",
    "for component_name, data_frames in frames_data.items():\n",
    "    for i, frame in enumerate(data_frames):\n",
    "        if i >= len(frames_data_all_components):\n",
    "            frames_data_all_components.append({})\n",
    "        \n",
    "        # Tambahkan prefix nama komponen ke nama kolom\n",
    "        for key, value in frame.items():\n",
    "            if key not in ['Frame', 'Folder Path', 'Label']:\n",
    "                frames_data_all_components[i][f'{component_name}-{key}'] = value\n",
    "            else:\n",
    "                frames_data_all_components[i][key] = value\n",
    "\n",
    "# Initialisasi dataframe dengan pandas untuk semua komponen\n",
    "df_all = pd.DataFrame(frames_data_all_components)\n",
    "\n",
    "# Inisialisasi nama file untuk dataset semua komponen\n",
    "nama_file_csv = f'{output_csv_dir}/nilai-fitur-all-component.csv'\n",
    "nama_file_xlsx = f'{output_excel_dir}/nilai-fitur-all-component.xlsx'\n",
    "\n",
    "# Hapus file output jika sudah ada\n",
    "if os.path.exists(nama_file_csv):\n",
    "    os.remove(nama_file_csv)\n",
    "if os.path.exists(nama_file_xlsx):\n",
    "    os.remove(nama_file_xlsx)\n",
    "\n",
    "# Simpan ke file CSV dan Excel\n",
    "df_all.to_csv(nama_file_csv, index=False, float_format=None)\n",
    "df_all.to_excel(nama_file_xlsx, index=False, float_format=None)\n",
    "\n",
    "print(\"Ekstraksi fitur selesai. Hasil telah disimpan di folder:\", pathDirectory['result_dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(A)menggunakan yang atas dulu\n",
    "# Import module yang digunakan\n",
    "from preprocessing.scarpping_component import extract_component_by_images\n",
    "from feature_extraction.poc import POC\n",
    "from feature_extraction.vektor import Vektor\n",
    "from feature_extraction.quadran import Quadran\n",
    "import re\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('(\\d+)', s)]\n",
    "\n",
    "# load model dan shape predictor untuk deteksi wajah\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"models/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "components_setup = {\n",
    "    'mulut': {\n",
    "        'object_name': 'Mouth',\n",
    "        'object_rectangle': {\"x_right\": 54, \"x_left\": 48, \"y_highest\": 52, \"y_lowest\": 57},\n",
    "        'pixel_shifting': {\"pixel_x\": 25, \"pixel_y\": 10},\n",
    "        'object_dimension': {'width': 140, 'height': 40}\n",
    "    },\n",
    "    'alis': {  # Gabungan alis kanan dan kiri menjadi satu komponen\n",
    "        'object_name': 'Eyebrows',\n",
    "        'object_rectangle': {\"x_right\": 26, \"x_left\": 17, \"y_highest\": 18, \"y_lowest\": 25},\n",
    "        'pixel_shifting': {\"pixel_x\": 20, \"pixel_y\": 15},\n",
    "        'object_dimension': {'width': 200, 'height': 40}\n",
    "    },\n",
    "    # 'mata_kiri': {\n",
    "    #     'object_name': 'eye_left',\n",
    "    #     'object_rectangle': {\"x_right\": 39, \"x_left\": 36, \"y_highest\": 38, \"y_lowest\": 40},\n",
    "    #     'pixel_shifting': {\"pixel_x\": 20, \"pixel_y\": 15},\n",
    "    #     'object_dimension': {'width': 81, 'height': 43}\n",
    "    # },\n",
    "    # 'mata_kanan': {\n",
    "    #     'object_name': 'eye_right',\n",
    "    #     'object_rectangle': {\"x_right\": 45, \"x_left\": 42, \"y_highest\": 43, \"y_lowest\": 47},\n",
    "    #     'pixel_shifting': {\"pixel_x\": 20, \"pixel_y\": 15},\n",
    "    #     'object_dimension': {'width': 81, 'height': 43}\n",
    "    # },\n",
    "    # 'alis_kiri': {\n",
    "    #     'object_name': 'eyebrow_left',\n",
    "    #     'object_rectangle': {\"x_right\": 21, \"x_left\": 17, \"y_highest\": 18, \"y_lowest\": 21},\n",
    "    #     'pixel_shifting': {\"pixel_x\": 15, \"pixel_y\": 5},\n",
    "    #     'object_dimension': {'width': 111, 'height': 28}\n",
    "    # },\n",
    "    # 'alis_kanan': {\n",
    "    #     'object_name': 'eyebrow_right',\n",
    "    #     'object_rectangle': {\"x_right\": 26, \"x_left\": 22, \"y_highest\": 25, \"y_lowest\": 22},\n",
    "    #     'pixel_shifting': {\"pixel_x\": 15, \"pixel_y\": 5},\n",
    "    #     'object_dimension': {'width': 111, 'height': 28}\n",
    "    # }\n",
    "}\n",
    "\n",
    "# Inisialisasi variabel untuk path url dan blok size\n",
    "blockSize = 7\n",
    "pathDirectory = {\n",
    "    \"base_path_dataset\": \"dataset\",\n",
    "    # \"base_dataset_input\" : \"dataset/casme_custom_from_one\",\n",
    "    \"base_dataset_input\" : \"dataset/training_images\",\n",
    "    \"result_dataset\" : \"output-baru\", \n",
    "    \"component_image\" : \"component_to_images\", \n",
    "    \"video_frame\" :\"video_to_images\"\n",
    "}\n",
    "\n",
    "# Inisialisasi variabel untuk menyimpan data dari masing-masing komponen\n",
    "frames_data_quadran = []\n",
    "frames_data_all_component = []\n",
    "frames_data_quadran_column = ['sumX', 'sumY', 'Tetha', 'Magnitude', 'JumlahQuadran']\n",
    "quadran_dimensions = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "frames_data = {component_name: [] for component_name in components_setup}\n",
    "total_blocks_components = {component_name: 0 for component_name in components_setup}\n",
    "data_blocks_first_image = {component_name: None for component_name in components_setup}\n",
    "index = {component_name: 0 for component_name in components_setup}\n",
    "\n",
    "# mapping disgust jadi apa disini\n",
    "\n",
    "# Hitung total blok dari masing-masing komponen lalu disetup kedalam total_blocks_components\n",
    "for component_name, component_info in components_setup.items():\n",
    "    total_blocks_components[component_name] = int((component_info['object_dimension']['width'] / blockSize) * (component_info['object_dimension']['height'] / blockSize))\n",
    "\n",
    "# Loop melalui setiap kunci dan nilai di pathDirectory dan hapus direktorinya\n",
    "for key, value in pathDirectory.items():\n",
    "    # Jika tidak mengandung base, hapus direktori sesuai dengan nilai yang ada\n",
    "    if not \"base\" in key:\n",
    "        shutil.rmtree(value, ignore_errors=True)\n",
    "\n",
    "# Looping folder label didalam base_dataset_input\n",
    "for foldername_label in os.listdir(pathDirectory['base_dataset_input']):\n",
    "    foldername_label_join_basepath = os.path.join(pathDirectory['base_dataset_input'], foldername_label)\n",
    "    # Jika folder nya tidak ada skip looping ini\n",
    "    if not os.path.isdir(foldername_label_join_basepath):\n",
    "        continue\n",
    "    \n",
    "    # Looping folder label didalam foldername_label_join_basepath \n",
    "    for foldername in os.listdir(foldername_label_join_basepath):\n",
    "        foldername_join_basepath = os.path.join(foldername_label_join_basepath, foldername)\n",
    "        # Jika folder nya tidak ada skip looping ini\n",
    "        if not os.path.isdir(foldername_join_basepath):\n",
    "            continue\n",
    "\n",
    "        # Reset variabel setiap kali mulai looping folder baru\n",
    "        data_blocks_first_image = {component_name: None for component_name in components_setup}\n",
    "        index = {component_name: 0 for component_name in components_setup}\n",
    "\n",
    "        # looping semua file yang ada didalam\n",
    "        for filename in sorted(os.listdir(foldername_join_basepath), key=natural_sort_key):\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"): \n",
    "\n",
    "                # Read path sesuai dengan foldername_join_basepath dijoin path dengan filename\n",
    "                image = cv2.imread(os.path.join(foldername_join_basepath, filename))\n",
    "                # Resize image ke ukuran yang diinginkan\n",
    "                # image = cv2.resize(image, (600, 500))\n",
    "                # Cpmvert image ke grayscale\n",
    "                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # Deteksi shape muka didalam grayscale image\n",
    "                rects = detector(gray)\n",
    "\n",
    "                if not index[component_name] == 0:\n",
    "                    # Buat variabel frames_data_all_component untuk menampung data current frame\n",
    "                    frame_data_all_component = {'Frame': f\"{index[component_name] + 1}({filename.split('.')[0]})\"}\n",
    "                    # Buat variabel frame_data_quadran untuk menampung data current frame\n",
    "                    frame_data_quadran = {'Frame': f\"{index[component_name] + 1}({filename.split('.')[0]})\"}\n",
    "\n",
    "                # Memproses rects untuk setiap bentuk wajah yang terdeteksi\n",
    "                for rect in rects:\n",
    "                    # Ambil bentuk wajah dalam bentuk shape sesuai dengan model predictor\n",
    "                    shape = predictor(gray, rect)\n",
    "                    # Memproses setiap komponen wajah\n",
    "                    for component_name, component_info in components_setup.items():\n",
    "                        # print(f\"\\n{foldername}-{filename.split('.')[0]}-{component_info['object_name']}:\")\n",
    "                        # Inisialisasi variabel sum_data_by_quadran untuk menyimpan data hasil quadran\n",
    "                        sum_data_by_quadran = {}\n",
    "                        # Buat variabel frame_data untuk menampung data current frame\n",
    "                        frame_data = {'Frame': f\"{index[component_name] + 1}({filename.split('.')[0]})\"}\n",
    "\n",
    "                        # Looping untuk setiap atribut dalam frames_data_quadran_column\n",
    "                        for column in frames_data_quadran_column:\n",
    "                            # Inisialisasi sub-dictionary untuk setiap atribut dalam frames_data_quadran_column yang defaultnya 0\n",
    "                            sum_data_by_quadran[column] = {quadrant: 0 for quadrant in quadran_dimensions}\n",
    "\n",
    "                        # Ambil data blok image dari return fungsi extract_component_by_images\n",
    "                        data_blocks_image_current = extract_component_by_images(\n",
    "                            image=image,\n",
    "                            shape=shape,\n",
    "                            frameName=filename.split(\".\")[0],\n",
    "                            objectName=component_info['object_name'],\n",
    "                            objectRectangle=component_info['object_rectangle'],\n",
    "                            pixelShifting=component_info['pixel_shifting'],\n",
    "                            objectDimension=component_info['object_dimension']\n",
    "                        )\n",
    "                        \n",
    "                        # Ambil frame pertama dari perulangan lalu simpan di variabel dan skip (lanjutkan ke frame berikut)\n",
    "                        if data_blocks_first_image[component_name] is None:\n",
    "                            # --- Setup bagian 4qmv Dataset ---\n",
    "                            # Inisialisasi data untuk setiap blok dan setiap kuadran dengan nilai sesuai sum_data_by_quadran\n",
    "                            # for quadrant in quadran_dimensions:\n",
    "                            #     for feature in frames_data_quadran_column:\n",
    "                            #         # Buat nama kolom dengan menggunakan template yang diberikan\n",
    "                            #         column_name = f\"{component_name}_{feature}_{quadrant}\"\n",
    "                            #         # Set value sum_data_by_quadran[feature][quadrant] ke frame_data_quadran sesuai column_name nya\n",
    "                            #         frame_data_quadran[column_name] = sum_data_by_quadran[feature][quadrant]\n",
    "\n",
    "                            # --- Setup bagian Nilai Fitur Dataset ---\n",
    "                            # Inisialisasi data untuk setiap blok\n",
    "                            # for i in range(total_blocks_components[component_name]):\n",
    "                                # Tambahkan data ke frame_data sesuai dengan indexnya\n",
    "                                # frame_data[f'X{i+1}'] = 0\n",
    "                                # frame_data[f'Y{i+1}'] = 0\n",
    "                                # frame_data[f'Tetha{i+1}'] = 0\n",
    "                                # frame_data[f'Magnitude{i+1}'] = 0\n",
    "                                # # Tambahkan data ke frame_data_all_component sesuai dengan indexnya\n",
    "                                # frame_data_all_component[f'{component_name}-X{i+1}'] = 0\n",
    "                                # frame_data_all_component[f'{component_name}-Y{i+1}'] = 0\n",
    "                                # frame_data_all_component[f'{component_name}-Tetha{i+1}'] = 0\n",
    "                                # frame_data_all_component[f'{component_name}-Magnitude{i+1}'] = 0\n",
    "\n",
    "                            # Append data frame ke list frames_data sesuai dengan component_name\n",
    "                            frames_data[component_name].append(frame_data)\n",
    "                            # Tambahkan kolom \"Folder Path\" dengan nilai folder saat ini\n",
    "                            frame_data['Folder Path'] = foldername\n",
    "                            # Tambahkan kolom \"Label\" dengan nilai label saat ini\n",
    "                            frame_data['Label'] = foldername_label\n",
    "                            # Set value data_blocks_first_image[component_name] ke data_blocks_image_current\n",
    "                            data_blocks_first_image[component_name] = data_blocks_image_current\n",
    "                            # Skip looping nya ke looping selanjutnya\n",
    "                            continue\n",
    "\n",
    "                        # # Tampilkan data block image current ke matplotlib\n",
    "                        # plt.imshow(np.uint8(data_blocks_image_current), cmap=\"gray\")\n",
    "\n",
    "                        # Inisiasi class POC\n",
    "                        initPOC = POC(data_blocks_first_image[component_name], data_blocks_image_current, blockSize) \n",
    "                        # Pemanggilan fungsi pocCalc() untuk menghitung nilai POC disetiap gambar\n",
    "                        valPOC = initPOC.getPOC() \n",
    "\n",
    "                        # Pemanggilan class dan method untuk menampilkan quiver / gambar panah\n",
    "                        initQuiv = Vektor(valPOC, blockSize)\n",
    "                        quivData = initQuiv.getVektor() \n",
    "\n",
    "                        # plt.quiver(quivData[:, 0], quivData[:, 1], quivData[:, 2], quivData[:, 3], scale=1, scale_units='xy', angles='xy', color=\"r\")    \n",
    "\n",
    "                        # # num = 0\n",
    "                        # for rect_def in valPOC[2]:\n",
    "                        #     x, y, width, height = rect_def\n",
    "                            \n",
    "                        #     rects = patches.Rectangle((x,y), width,height, edgecolor='r', facecolor='none') \n",
    "                        #     plt.gca().add_patch(rects)\n",
    "                            \n",
    "                        #     # plt.text(x,y,f'({num})', color=\"blue\") \n",
    "                        #     # num += 1\n",
    "\n",
    "                        # Pemanggilan class untuk mengeluarkan nilai karakteristik vektor\n",
    "                        # blok ke, x,y,tetha, magnitude, dan quadran ke\n",
    "                        initQuadran = Quadran(quivData) \n",
    "                        quadran = initQuadran.getQuadran()\n",
    "\n",
    "                        # print(tabulate(quadran, headers=['Blok Ke', 'X', 'Y', 'Tetha', 'Magnitude', 'Quadran Ke']))\n",
    "                        # plt.axis('on') \n",
    "                        # plt.show() \n",
    "\n",
    "                        # Update frame_data dengan data quadran\n",
    "                        for i, quad in enumerate(quadran):\n",
    "                            # --- Setup bagian Nilai Fitur Dataset ---\n",
    "                            # Set data kedalam frame_data sesuai column nya\n",
    "                            frame_data[f'X{i+1}'] = quad[1]\n",
    "                            frame_data[f'Y{i+1}'] = quad[2]\n",
    "                            frame_data[f'Tetha{i+1}'] = quad[3]\n",
    "                            frame_data[f'Magnitude{i+1}'] = quad[4]\n",
    "\n",
    "                            # Set data kedalam frame_data_all_component sesuai columnnya\n",
    "                            frame_data_all_component[f'{component_name}-X{i+1}'] = quad[1]\n",
    "                            frame_data_all_component[f'{component_name}-Y{i+1}'] = quad[2]\n",
    "                            frame_data_all_component[f'{component_name}-Tetha{i+1}'] = quad[3]\n",
    "                            frame_data_all_component[f'{component_name}-Magnitude{i+1}'] = quad[4]\n",
    "\n",
    "                            # --- Setup bagian 4qmv Dataset ---\n",
    "                            # Cek apakah quad[5] ada didalam array quadran_dimensions\n",
    "                            if quad[5] in quadran_dimensions:\n",
    "                                # Tambahkan nilai quad[1] ke sumX pada kuadran yang sesuai\n",
    "                                sum_data_by_quadran['sumX'][quad[5]] += quad[1]\n",
    "                                # Tambahkan nilai quad[2] ke sumY pada kuadran yang sesuai\n",
    "                                sum_data_by_quadran['sumY'][quad[5]] += quad[2]\n",
    "                                # Tambahkan nilai quad[3] ke Tetha pada kuadran yang sesuai\n",
    "                                sum_data_by_quadran['Tetha'][quad[5]] += quad[3]\n",
    "                                # Tambahkan nilai quad[4] ke Magnitude pada kuadran yang sesuai\n",
    "                                sum_data_by_quadran['Magnitude'][quad[5]] += quad[4]\n",
    "                                # Tambahkan jumlah quadran sesuai dengan quad[5] ke JumlahQuadran pada kuadran yang sesuai\n",
    "                                sum_data_by_quadran['JumlahQuadran'][quad[5]] += 1\n",
    "                        \n",
    "                        # --- Setup bagian Nilai Fitur Dataset ---\n",
    "                        # Append data frame ke list\n",
    "                        frames_data[component_name].append(frame_data)\n",
    "                        # Tambahkan kolom \"Folder Path\" dengan nilai folder saat ini\n",
    "                        frame_data['Folder Path'] = foldername\n",
    "                        # Tambahkan kolom \"Label\" dengan nilai label saat ini\n",
    "                        frame_data['Label'] = foldername_label\n",
    "                        # disini set slabel nua ke hasil mapping\n",
    "\n",
    "                        # --- Setup bagian 4qmv Dataset ---\n",
    "                        # Inisialisasi data untuk setiap blok dan setiap kuadran dengan nilai sesuai sum_data_by_quadran\n",
    "                        for quadran in quadran_dimensions:\n",
    "                            for feature in frames_data_quadran_column:\n",
    "                                # Buat nama kolom dengan menggunakan template yang diberikan\n",
    "                                column_name = f\"{component_name}_{feature}_{quadran}\"\n",
    "                                # Set value sum_data_by_quadran[feature][quadran] ke frame_data_quadran sesuai column_name nya\n",
    "                                frame_data_quadran[column_name] = sum_data_by_quadran[feature][quadran]\n",
    "\n",
    "                # --- Setup bagian 4qmv Dataset ---\n",
    "                if not index[component_name] == 0:\n",
    "                    # Append data frame ke list frames_data_quadran untuk 4qmv\n",
    "                    frames_data_quadran.append(frame_data_quadran)\n",
    "                    # print(\"Frame Quadran\", frame_data_quadran)\n",
    "                    # Tambahkan kolom \"Folder Path\" dengan nilai folder saat ini\n",
    "                    frame_data_quadran['Folder Path'] = foldername\n",
    "                    # Tambahkan kolom \"Label\" dengan nilai label saat ini\n",
    "                    frame_data_quadran['Label'] = foldername_label\n",
    "                    # disini set slabel nua ke hasil mapping\n",
    "\n",
    "                if not index[component_name] == 0:\n",
    "                    # --- Setup bagian frames data all component Dataset ---\n",
    "                    # Append data frame ke list frames_data_quadran untuk 4qmv\n",
    "                    frames_data_all_component.append(frame_data_all_component)\n",
    "                    # print(\"Frame Quadran\", frame_data_quadran)\n",
    "                    # Tambahkan kolom \"Folder Path\" dengan nilai folder saat ini\n",
    "                    frame_data_all_component['Folder Path'] = foldername\n",
    "                    # Tambahkan kolom \"Label\" dengan nilai label saat ini\n",
    "                    frame_data_all_component['Label'] = foldername_label\n",
    "                    # disini set slabel nua ke hasil mapping\n",
    "\n",
    "                # Update index per component_name\n",
    "                index[component_name] += 1\n",
    "\n",
    "# Membuat direktori jika belum ada untuk outputnya\n",
    "output_csv_dir = os.path.join(pathDirectory['result_dataset'], 'csv')\n",
    "output_excel_dir = os.path.join(pathDirectory['result_dataset'], 'excel')\n",
    "os.makedirs(output_csv_dir, exist_ok=True)\n",
    "os.makedirs(output_excel_dir, exist_ok=True)\n",
    "\n",
    "# Hapus file output dari semua tipe dataset baik csv dan xlsx jika ada (Nilai fitur)\n",
    "for component_name in components_setup:\n",
    "    csv_file_path = os.path.join(pathDirectory['result_dataset'], 'csv', f'{component_name}.csv')\n",
    "    xlsx_file_path = os.path.join(pathDirectory['result_dataset'], 'excel', f'{component_name}.xlsx')\n",
    "    if os.path.exists(csv_file_path):\n",
    "        os.remove(csv_file_path)\n",
    "    if os.path.exists(xlsx_file_path):\n",
    "        os.remove(xlsx_file_path)\n",
    "\n",
    "# Export dataframe ke dalam file csv dan xlsx sesuai dengan masing-masing component untuk fitur ekstrasi biasa\n",
    "for component_name, data_frame in frames_data.items():\n",
    "    # Initialisasi dataframe dengan pandas\n",
    "    df = pd.DataFrame(data_frame)\n",
    "    \n",
    "    # Simpan ke file CSV\n",
    "    nama_file_csv = f'{output_csv_dir}/{component_name}.csv'\n",
    "    df.to_csv(nama_file_csv, index=False, float_format=None)\n",
    "    \n",
    "    # Simpan ke file Excel\n",
    "    nama_file_xlsx = f'{output_excel_dir}/{component_name}.xlsx'\n",
    "    df.to_excel(nama_file_xlsx, index=False, float_format=None)\n",
    "\n",
    "# Export dataframe ke dalam file csv dan xlsx sesuai frames_data_quadran untuk fitur ekstraksi 4qmv\n",
    "# Initialisasi dataframe dengan pandas\n",
    "df_4qmv = pd.DataFrame(frames_data_quadran)\n",
    "\n",
    "# Inisialisasi nama file untuk dataset 4qmv\n",
    "nama_file_csv = f'{output_csv_dir}/4qmv-all-component.csv'\n",
    "nama_file_xlsx = f'{output_excel_dir}/4qmv-all-component.xlsx'\n",
    "\n",
    "# Hapus file output dari semua tipe dataset baik csv dan xlsx jika ada (4qmv)\n",
    "if os.path.exists(nama_file_csv):\n",
    "    os.remove(nama_file_csv)\n",
    "if os.path.exists(nama_file_xlsx):\n",
    "    os.remove(nama_file_xlsx)\n",
    "\n",
    "# Simpan ke file CSV\n",
    "df_4qmv.to_csv(nama_file_csv, index=False, float_format=None)\n",
    "\n",
    "# Simpan ke file Excel\n",
    "df_4qmv.to_excel(nama_file_xlsx, index=False, float_format=None)\n",
    "\n",
    "# Export dataframe ke dalam file csv dan xlsx sesuai frames_data_all_component untuk fitur ekstraksi nilai fiturnya\n",
    "# Initialisasi dataframe dengan pandas\n",
    "df_fitur_all = pd.DataFrame(frames_data_all_component)\n",
    "\n",
    "# Inisialisasi nama file untuk dataset 4qmv\n",
    "nama_file_csv = f'{output_csv_dir}/nilai-fitur-all-component.csv'\n",
    "nama_file_xlsx = f'{output_excel_dir}/nilai-fitur-all-component.xlsx'\n",
    "\n",
    "# Hapus file output dari semua tipe dataset baik csv dan xlsx jika ada (4qmv)\n",
    "if os.path.exists(nama_file_csv):\n",
    "    os.remove(nama_file_csv)\n",
    "if os.path.exists(nama_file_xlsx):\n",
    "    os.remove(nama_file_xlsx)\n",
    "\n",
    "# Simpan ke file CSV\n",
    "df_fitur_all.to_csv(nama_file_csv, index=False, float_format=None)\n",
    "\n",
    "# Simpan ke file Excel\n",
    "df_fitur_all.to_excel(nama_file_xlsx, index=False, float_format=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def perform_pca_feature_selection(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Perform PCA feature selection on CSV files in the input directory\n",
    "    and evaluate classification accuracy\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dir : str\n",
    "        Directory containing input CSV files\n",
    "    output_dir : str\n",
    "        Directory to save PCA results\n",
    "    \"\"\"\n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all CSV files in the input directory\n",
    "    csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            # Full path to the CSV file\n",
    "            input_path = os.path.join(input_dir, csv_file)\n",
    "            \n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(input_path)\n",
    "            \n",
    "            # Check for NaN values and print summary\n",
    "            nan_count = df.isna().sum().sum()\n",
    "            if nan_count > 0:\n",
    "                print(f\"Warning: {csv_file} contains {nan_count} NaN values. Imputing missing values.\")\n",
    "                \n",
    "            # Remove non-numeric columns for PCA\n",
    "            numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "            X = df[numeric_columns]\n",
    "            \n",
    "            # Separate features from metadata columns\n",
    "            metadata_columns = []\n",
    "            for col in ['Frame', 'Folder Path']:\n",
    "                if col in df.columns:\n",
    "                    metadata_columns.append(col)\n",
    "                    \n",
    "            label_column = 'Label'\n",
    "            \n",
    "            # Check if label column exists in the DataFrame\n",
    "            if label_column not in df.columns:\n",
    "                print(f\"Warning: '{label_column}' column not found in {csv_file}. Skipping accuracy calculation.\")\n",
    "                continue\n",
    "                \n",
    "            metadata = df[metadata_columns] if metadata_columns else pd.DataFrame()\n",
    "            y = df[label_column]\n",
    "            \n",
    "            # Handle missing values with SimpleImputer\n",
    "            imputer = SimpleImputer(strategy='mean')\n",
    "            X_imputed = imputer.fit_transform(X)\n",
    "            \n",
    "            # Standardize the features\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X_imputed)\n",
    "            \n",
    "            # Perform PCA\n",
    "            # We'll use 95% explained variance as our criteria\n",
    "            pca = PCA(n_components=0.95)\n",
    "            X_pca = pca.fit_transform(X_scaled)\n",
    "            \n",
    "            # Create a DataFrame with PCA results\n",
    "            pca_columns = [f'PC{i+1}' for i in range(X_pca.shape[1])]\n",
    "            df_pca = pd.DataFrame(X_pca, columns=pca_columns)\n",
    "            \n",
    "            # Add back the metadata columns and label\n",
    "            if not metadata.empty:\n",
    "                df_pca = pd.concat([metadata, df_pca], axis=1)\n",
    "            df_pca[label_column] = y\n",
    "            \n",
    "            # Plot explained variance ratio\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "                    pca.explained_variance_ratio_)\n",
    "            plt.xlabel('Principal Components')\n",
    "            plt.ylabel('Explained Variance Ratio')\n",
    "            plt.title(f'PCA Explained Variance - {csv_file}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, f'pca_variance_{csv_file.replace(\".csv\", \".png\")}'))\n",
    "            plt.close()\n",
    "            \n",
    "            # Save PCA results\n",
    "            output_csv = os.path.join(output_dir, f'pca_{csv_file}')\n",
    "            df_pca.to_csv(output_csv, index=False)\n",
    "            \n",
    "            # Calculate accuracy using a classifier (RandomForest)\n",
    "            # Split data into training and testing sets\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_pca, y, test_size=0.3, random_state=42, stratify=y if len(np.unique(y)) > 1 else None\n",
    "            )\n",
    "            \n",
    "            # Train a classifier\n",
    "            clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            # Generate and save classification report\n",
    "            report = classification_report(y_test, y_pred, output_dict=True)\n",
    "            report_df = pd.DataFrame(report).transpose()\n",
    "            report_df.to_csv(os.path.join(output_dir, f'classification_report_{csv_file}'))\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"PCA Results for {csv_file}:\")\n",
    "            print(f\"  Original Features: {X.shape[1]}\")\n",
    "            print(f\"  Reduced Features: {X_pca.shape[1]}\")\n",
    "            print(f\"  Cumulative Explained Variance: {pca.explained_variance_ratio_.sum() * 100:.2f}%\")\n",
    "            print(f\"  Classification Accuracy: {accuracy * 100:.2f}%\")\n",
    "            print(f\"  Output saved to: {output_csv}\\n\")\n",
    "            \n",
    "            # Compare with original features accuracy\n",
    "            clf_original = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "                X_scaled, y, test_size=0.3, random_state=42, stratify=y if len(np.unique(y)) > 1 else None\n",
    "            )\n",
    "            clf_original.fit(X_train_orig, y_train_orig)\n",
    "            y_pred_orig = clf_original.predict(X_test_orig)\n",
    "            accuracy_orig = accuracy_score(y_test_orig, y_pred_orig)\n",
    "            \n",
    "            print(f\"  Original Features Accuracy: {accuracy_orig * 100:.2f}%\")\n",
    "            print(f\"  Accuracy Difference: {(accuracy - accuracy_orig) * 100:.2f}%\\n\")\n",
    "            \n",
    "            # Plot accuracy comparison\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.bar(['Original Features', 'PCA Features'], [accuracy_orig, accuracy], color=['blue', 'orange'])\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title(f'Accuracy Comparison - {csv_file}')\n",
    "            plt.ylim(0, 1.1)\n",
    "            for i, v in enumerate([accuracy_orig, accuracy]):\n",
    "                plt.text(i, v + 0.05, f'{v:.2%}', ha='center')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, f'accuracy_comparison_{csv_file.replace(\".csv\", \".png\")}'))\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {csv_file}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Directories for input and output\n",
    "input_dir = 'output-baru/csv'  # Update this to match your directory structure\n",
    "output_dir = 'output-baru/pca_results'\n",
    "\n",
    "# Run PCA feature selection with accuracy measurement\n",
    "perform_pca_feature_selection(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def perform_rfe_feature_selection(input_dir, output_dir, n_features_to_select=None, step=1):\n",
    "    \"\"\"\n",
    "    Perform RFE (Recursive Feature Elimination) feature selection on CSV files in the input directory\n",
    "    and evaluate classification accuracy\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dir : str\n",
    "        Directory containing input CSV files\n",
    "    output_dir : str\n",
    "        Directory to save RFE results\n",
    "    n_features_to_select : int or None\n",
    "        Number of features to select. If None, half of the features will be selected.\n",
    "    step : int\n",
    "        Number of features to remove at each iteration\n",
    "    \"\"\"\n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all CSV files in the input directory\n",
    "    csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            # Full path to the CSV file\n",
    "            input_path = os.path.join(input_dir, csv_file)\n",
    "            \n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(input_path)\n",
    "            \n",
    "            # Check for NaN values and print summary\n",
    "            nan_count = df.isna().sum().sum()\n",
    "            if nan_count > 0:\n",
    "                print(f\"Warning: {csv_file} contains {nan_count} NaN values. Imputing missing values.\")\n",
    "                \n",
    "            # Remove non-numeric columns for RFE\n",
    "            numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "            X = df[numeric_columns]\n",
    "            \n",
    "            # Separate features from metadata columns\n",
    "            metadata_columns = []\n",
    "            for col in ['Frame', 'Folder Path']:\n",
    "                if col in df.columns:\n",
    "                    metadata_columns.append(col)\n",
    "                    \n",
    "            label_column = 'Label'\n",
    "            \n",
    "            # Check if label column exists in the DataFrame\n",
    "            if label_column not in df.columns:\n",
    "                print(f\"Warning: '{label_column}' column not found in {csv_file}. Skipping accuracy calculation.\")\n",
    "                continue\n",
    "                \n",
    "            metadata = df[metadata_columns] if metadata_columns else pd.DataFrame()\n",
    "            y = df[label_column]\n",
    "            \n",
    "            # Remove any metadata columns from features if they were included in numeric_columns\n",
    "            X = X.drop(columns=[col for col in metadata_columns if col in X.columns], errors='ignore')\n",
    "            \n",
    "            # Handle missing values with SimpleImputer\n",
    "            imputer = SimpleImputer(strategy='mean')\n",
    "            X_imputed = imputer.fit_transform(X)\n",
    "            \n",
    "            # Standardize the features\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X_imputed)\n",
    "            \n",
    "            # Create column names for scaled data\n",
    "            feature_names = X.columns.tolist()\n",
    "            \n",
    "            # Set default n_features_to_select if None\n",
    "            if n_features_to_select is None:\n",
    "                n_features_to_select = max(1, X.shape[1] // 2)  # Select half of the features by default\n",
    "                \n",
    "            # Create a base estimator (Random Forest classifier)\n",
    "            estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            \n",
    "            # Create RFE with the estimator\n",
    "            rfe = RFE(estimator=estimator, n_features_to_select=n_features_to_select, step=step)\n",
    "            \n",
    "            # Fit RFE\n",
    "            rfe.fit(X_scaled, y)\n",
    "            \n",
    "            # Get selected features\n",
    "            selected_features_mask = rfe.support_\n",
    "            selected_features = [feature for feature, selected in zip(feature_names, selected_features_mask) if selected]\n",
    "            \n",
    "            # Transform data using selected features\n",
    "            X_rfe = X_scaled[:, selected_features_mask]\n",
    "            \n",
    "            # Print feature ranking\n",
    "            feature_ranking = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Ranking': rfe.ranking_,\n",
    "                'Selected': selected_features_mask\n",
    "            }).sort_values('Ranking')\n",
    "            \n",
    "            # Save feature ranking\n",
    "            feature_ranking.to_csv(os.path.join(output_dir, f'feature_ranking_{csv_file}'), index=False)\n",
    "            \n",
    "            # Create a DataFrame with RFE results\n",
    "            df_rfe = pd.DataFrame(X_rfe, columns=[f'Feature_{i+1}' for i in range(X_rfe.shape[1])])\n",
    "            \n",
    "            # Add original selected feature names as metadata\n",
    "            feature_mapping = pd.DataFrame({\n",
    "                'RFE_Feature': [f'Feature_{i+1}' for i in range(len(selected_features))],\n",
    "                'Original_Feature': selected_features\n",
    "            })\n",
    "            feature_mapping.to_csv(os.path.join(output_dir, f'feature_mapping_{csv_file}'), index=False)\n",
    "            \n",
    "            # Add back the metadata columns and label\n",
    "            if not metadata.empty:\n",
    "                df_rfe = pd.concat([metadata, df_rfe], axis=1)\n",
    "            df_rfe[label_column] = y\n",
    "            \n",
    "            # Save RFE results\n",
    "            output_csv = os.path.join(output_dir, f'rfe_{csv_file}')\n",
    "            df_rfe.to_csv(output_csv, index=False)\n",
    "            \n",
    "            # Calculate accuracy using the classifier\n",
    "            # Split data into training and testing sets\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_rfe, y, test_size=0.3, random_state=42, stratify=y if len(np.unique(y)) > 1 else None\n",
    "            )\n",
    "            \n",
    "            # Train the classifier\n",
    "            clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            # Generate and save classification report\n",
    "            report = classification_report(y_test, y_pred, output_dict=True)\n",
    "            report_df = pd.DataFrame(report).transpose()\n",
    "            report_df.to_csv(os.path.join(output_dir, f'classification_report_{csv_file}'))\n",
    "            \n",
    "            # Generate confusion matrix\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.title(f'Confusion Matrix - {csv_file}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, f'confusion_matrix_{csv_file.replace(\".csv\", \".png\")}'))\n",
    "            plt.close()\n",
    "            \n",
    "            # Compare with original features accuracy\n",
    "            X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "                X_scaled, y, test_size=0.3, random_state=42, stratify=y if len(np.unique(y)) > 1 else None\n",
    "            )\n",
    "            clf_original = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            clf_original.fit(X_train_orig, y_train_orig)\n",
    "            y_pred_orig = clf_original.predict(X_test_orig)\n",
    "            accuracy_orig = accuracy_score(y_test_orig, y_pred_orig)\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"RFE Results for {csv_file}:\")\n",
    "            print(f\"  Original Features: {X.shape[1]}\")\n",
    "            print(f\"  Selected Features: {len(selected_features)}\")\n",
    "            print(f\"  Selected Feature Names: {selected_features}\")\n",
    "            print(f\"  Classification Accuracy with RFE: {accuracy * 100:.2f}%\")\n",
    "            print(f\"  Original Features Accuracy: {accuracy_orig * 100:.2f}%\")\n",
    "            print(f\"  Accuracy Difference: {(accuracy - accuracy_orig) * 100:.2f}%\")\n",
    "            print(f\"  Output saved to: {output_csv}\\n\")\n",
    "            \n",
    "            # Plot accuracy comparison\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.bar(['Original Features', 'RFE Selected Features'], [accuracy_orig, accuracy], color=['blue', 'green'])\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title(f'Accuracy Comparison - {csv_file}')\n",
    "            plt.ylim(0, 1.1)\n",
    "            for i, v in enumerate([accuracy_orig, accuracy]):\n",
    "                plt.text(i, v + 0.05, f'{v:.2%}', ha='center')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, f'accuracy_comparison_{csv_file.replace(\".csv\", \".png\")}'))\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot feature importance of selected features\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            importances = clf.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            plt.bar(range(len(importances)), importances[indices])\n",
    "            plt.xticks(range(len(importances)), [f'Feature_{i+1}' for i in indices], rotation=90)\n",
    "            plt.title(f'Feature Importance of Selected Features - {csv_file}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, f'feature_importance_{csv_file.replace(\".csv\", \".png\")}'))\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {csv_file}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Directories for input and output\n",
    "input_dir = 'output-baru/csv'  # Update this to match your directory structure\n",
    "output_dir = 'output-baru/rfe_results'\n",
    "\n",
    "# Run RFE feature selection\n",
    "# You can adjust n_features_to_select to set the number of features you want to keep\n",
    "# If set to None, it will select half of the features by default\n",
    "perform_rfe_feature_selection(input_dir, output_dir, n_features_to_select=None, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_raw_data_accuracy(input_dir, results_dir):\n",
    "    \"\"\"\n",
    "    Menghitung akurasi dari raw data CSV\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dir : str\n",
    "        Direktori yang berisi file raw data CSV\n",
    "    results_dir : str\n",
    "        Direktori untuk menyimpan hasil analisis akurasi\n",
    "    \"\"\"\n",
    "    # Buat direktori hasil jika belum ada\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Cari semua file CSV di direktori input\n",
    "    csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    # Buat file ringkasan untuk semua hasil\n",
    "    summary_df = pd.DataFrame(columns=['File', 'Jumlah_Fitur', 'Akurasi'])\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            print(f\"Memproses file raw data: {csv_file}\")\n",
    "            \n",
    "            # Jalur penuh ke file CSV\n",
    "            input_path = os.path.join(input_dir, csv_file)\n",
    "            \n",
    "            # Baca file CSV\n",
    "            df = pd.read_csv(input_path)\n",
    "            \n",
    "            # Cari kolom label\n",
    "            label_column = 'Label'\n",
    "            if label_column not in df.columns:\n",
    "                print(f\"Peringatan: Kolom '{label_column}' tidak ditemukan di {csv_file}. Mencoba menemukan kolom label lain...\")\n",
    "                # Coba cari kolom label lain\n",
    "                possible_labels = [col for col in df.columns if 'label' in col.lower() or 'kelas' in col.lower() or 'class' in col.lower()]\n",
    "                if possible_labels:\n",
    "                    label_column = possible_labels[0]\n",
    "                    print(f\"Menggunakan kolom '{label_column}' sebagai label.\")\n",
    "                else:\n",
    "                    print(f\"Tidak dapat menemukan kolom label di {csv_file}. Melewati file ini.\")\n",
    "                    continue\n",
    "            \n",
    "            # Pisahkan fitur dan label\n",
    "            y = df[label_column]\n",
    "            \n",
    "            # Pisahkan kolom metadata dan non-numerik\n",
    "            metadata_columns = []\n",
    "            for col in ['Frame', 'Folder Path', label_column]:\n",
    "                if col in df.columns:\n",
    "                    metadata_columns.append(col)\n",
    "                    \n",
    "            # Hanya gunakan kolom numerik untuk model\n",
    "            numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "            X = df[numeric_columns]\n",
    "            \n",
    "            # Hapus kolom metadata dari X jika ada\n",
    "            X = X.drop(columns=[col for col in metadata_columns if col in X.columns], errors='ignore')\n",
    "            \n",
    "            # Cek dan tangani nilai NaN\n",
    "            nan_count = X.isna().sum().sum()\n",
    "            if nan_count > 0:\n",
    "                print(f\"  Peringatan: Data mengandung {nan_count} nilai NaN. Melakukan imputasi...\")\n",
    "                imputer = SimpleImputer(strategy='mean')\n",
    "                X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "            \n",
    "            # Standardisasi fitur\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "            \n",
    "            # Bagi data menjadi training dan testing\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_scaled, y, test_size=0.3, random_state=42, stratify=y if len(np.unique(y)) > 1 else None\n",
    "            )\n",
    "            \n",
    "            # Latih model Random Forest\n",
    "            clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            # Prediksi dan hitung akurasi\n",
    "            y_pred = clf.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            # Buat dan simpan classification report\n",
    "            report = classification_report(y_test, y_pred, output_dict=True)\n",
    "            report_df = pd.DataFrame(report).transpose()\n",
    "            report_df.to_csv(os.path.join(results_dir, f'raw_accuracy_report_{csv_file}'))\n",
    "            \n",
    "            # Tampilkan ringkasan\n",
    "            print(f\"Hasil Akurasi Raw Data untuk {csv_file}:\")\n",
    "            print(f\"  Jumlah Fitur: {X.shape[1]}\")\n",
    "            print(f\"  Akurasi: {accuracy * 100:.2f}%\")\n",
    "            print(f\"  Report disimpan di: {os.path.join(results_dir, f'raw_accuracy_report_{csv_file}')}\\n\")\n",
    "            \n",
    "            # Tambahkan ke dataframe ringkasan\n",
    "            summary_df = pd.concat([summary_df, pd.DataFrame({\n",
    "                'File': [csv_file],\n",
    "                'Jumlah_Fitur': [X.shape[1]],\n",
    "                'Akurasi': [accuracy * 100]\n",
    "            })], ignore_index=True)\n",
    "            \n",
    "            # Visualisasi fitur importance (top 20 jika terlalu banyak fitur)\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            importances = clf.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            \n",
    "            # Tunjukkan maksimal 20 fitur paling penting\n",
    "            top_n = min(20, len(importances))\n",
    "            plt.bar(range(top_n), importances[indices[:top_n]])\n",
    "            plt.xticks(range(top_n), X.columns[indices[:top_n]], rotation=90)\n",
    "            plt.title(f'20 Fitur Terpenting - {csv_file}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(results_dir, f'raw_feature_importance_{csv_file.replace(\".csv\", \".png\")}'))\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saat memproses raw data {csv_file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Simpan ringkasan akurasi untuk semua file\n",
    "    if not summary_df.empty:\n",
    "        summary_df = summary_df.sort_values('Akurasi', ascending=False)\n",
    "        summary_df.to_csv(os.path.join(results_dir, 'raw_summary_accuracy.csv'), index=False)\n",
    "        print(f\"Ringkasan akurasi raw data disimpan di: {os.path.join(results_dir, 'raw_summary_accuracy.csv')}\")\n",
    "        \n",
    "        # Visualisasi perbandingan akurasi\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.bar(summary_df['File'], summary_df['Akurasi'], color='lightgreen')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.ylabel('Akurasi (%)')\n",
    "        plt.title('Perbandingan Akurasi Raw Data')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(results_dir, 'raw_accuracy_comparison.png'))\n",
    "        plt.close()\n",
    "\n",
    "# Direktori yang berisi raw data CSV dan direktori untuk menyimpan hasil analisis\n",
    "input_dir = 'output-baru/csv'  # Sesuaikan dengan direktori raw data Anda\n",
    "results_dir = 'output-baru/accuracy_analysis'\n",
    "\n",
    "# Hitung akurasi dari raw data\n",
    "calculate_raw_data_accuracy(input_dir, results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def perform_svm_classification(input_dir, results_dir):\n",
    "    \"\"\"\n",
    "    Perform SVM Classification on feature-selected datasets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dir : str\n",
    "        Directory containing feature-selected CSV files\n",
    "    results_dir : str\n",
    "        Directory to save classification results\n",
    "    \"\"\"\n",
    "    # Create results directory\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all CSV files in the input directory\n",
    "    csv_files = [f for f in os.listdir(input_dir) if f.startswith('rfe_') and f.endswith('.csv')]\n",
    "    \n",
    "    # Summary of results\n",
    "    classification_summary = []\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        # Full path to the CSV file\n",
    "        input_path = os.path.join(input_dir, csv_file)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(input_path)\n",
    "        \n",
    "        # Separate features and target\n",
    "        metadata_columns = ['Frame', 'Folder Path', 'Label']\n",
    "        X = df.drop(columns=metadata_columns)\n",
    "        y = df['Label']\n",
    "        \n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=0.2,  # 80% training, 20% testing\n",
    "            random_state=42,  # for reproducibility\n",
    "            stratify=y  # ensure proportional class distribution\n",
    "        )\n",
    "        \n",
    "        # Scale the features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Initialize and train SVM\n",
    "        svm = SVC(\n",
    "            kernel='rbf',  # Radial Basis Function kernel\n",
    "            C=1.0,  # Regularization parameter\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_scores = cross_val_score(svm, X_train_scaled, y_train, cv=5)\n",
    "        \n",
    "        # Fit the model\n",
    "        svm.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = svm.predict(X_test_scaled)\n",
    "        \n",
    "        # Generate classification report\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Visualize Confusion Matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix - {csv_file}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(results_dir, f'confusion_matrix_{csv_file.replace(\".csv\", \".png\")}'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Store summary\n",
    "        summary_entry = {\n",
    "            'Dataset': csv_file,\n",
    "            'Accuracy': report['accuracy'],\n",
    "            'Macro Avg F1-Score': report['macro avg']['f1-score'],\n",
    "            'Cross-Val Mean': cv_scores.mean(),\n",
    "            'Cross-Val Std': cv_scores.std()\n",
    "        }\n",
    "        classification_summary.append(summary_entry)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nClassification Results for {csv_file}:\")\n",
    "        print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "        print(f\"Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Save summary to CSV\n",
    "    summary_df = pd.DataFrame(classification_summary)\n",
    "    summary_df.to_csv(os.path.join(results_dir, 'classification_summary.csv'), index=False)\n",
    "    print(\"\\nClassification summary saved to classification_summary.csv\")\n",
    "\n",
    "# Directories for input and output\n",
    "input_dir = 'output-baru/rfe_results'\n",
    "results_dir = 'output-baru/svm_classification_results'\n",
    "\n",
    "# Run SVM Classification\n",
    "perform_svm_classification(input_dir, results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output-baru/csv/nilai-fitur-all-component.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m classifier \u001b[38;5;241m=\u001b[39m SVMClassifier(dataset_file, label_column, except_feature_column\u001b[38;5;241m=\u001b[39mexcept_feature_columns)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Load, split, train, evaluate and save the model\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m classifier\u001b[38;5;241m.\u001b[39msplit_data(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m     15\u001b[0m classifier\u001b[38;5;241m.\u001b[39mtrain_model(autoParams\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\skripsi-ekspresi-mikro\\services\\ml-pipeline\\clasification\\svm.py:29\u001b[0m, in \u001b[0;36mSVMClassifier.load_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 29\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_file)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_column \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexcept_feature_column \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexcept_feature_column \u001b[38;5;241m==\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m]):\n",
      "File \u001b[1;32mc:\\Users\\Fina Orivia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Fina Orivia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Fina Orivia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Fina Orivia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Fina Orivia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output-baru/csv/nilai-fitur-all-component.csv'"
     ]
    }
   ],
   "source": [
    "# Random Sampling Fitur All Component\n",
    "from clasification.svm import SVMClassifier\n",
    "\n",
    "# Load the dataset and initialize the classifier\n",
    "dataset_file = 'output-baru/csv/nilai-fitur-all-component.csv'\n",
    "label_column = 'Label'\n",
    "except_feature_columns = ['Frame', 'Folder Path', 'Label']  # Columns to exclude\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier = SVMClassifier(dataset_file, label_column, except_feature_column=except_feature_columns)\n",
    "\n",
    "# Load, split, train, evaluate and save the model\n",
    "classifier.load_data()\n",
    "classifier.split_data(test_size=0.2)\n",
    "classifier.train_model(autoParams=True)\n",
    "classifier.evaluate_model()\n",
    "classifier.save_model('svm_model_random_sampling.joblib', 'label_encoder_random_sampling.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def perform_svm_classification(input_dir, results_dir):\n",
    "    \"\"\"\n",
    "    Perform SVM Classification menggunakan berbagai metode seleksi fitur\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dir : str\n",
    "        Direktori berisi file CSV\n",
    "    results_dir : str\n",
    "        Direktori untuk menyimpan hasil klasifikasi\n",
    "    \"\"\"\n",
    "    # Buat direktori hasil\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Daftar file CSV\n",
    "    csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    # Ringkasan hasil klasifikasi\n",
    "    classification_summary = []\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        # Path lengkap file CSV\n",
    "        input_path = os.path.join(input_dir, csv_file)\n",
    "        \n",
    "        # Baca file CSV\n",
    "        df = pd.read_csv(input_path)\n",
    "        \n",
    "        # Pisahkan fitur dan target\n",
    "        metadata_columns = ['Frame', 'Folder Path', 'Label']\n",
    "        X = df.drop(columns=metadata_columns)\n",
    "        y = df['Label']\n",
    "        \n",
    "        # Metode seleksi fitur yang akan diuji\n",
    "        feature_selection_methods = [\n",
    "            ('Raw Data', X),\n",
    "            ('RFE', perform_rfe_selection(X, y)),\n",
    "            ('PCA', perform_pca_selection(X))\n",
    "        ]\n",
    "        \n",
    "        for method_name, X_selected in feature_selection_methods:\n",
    "            # Bagi data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_selected, y, \n",
    "                test_size=0.2,  \n",
    "                random_state=42,  \n",
    "                stratify=y  \n",
    "            )\n",
    "            \n",
    "            # Skala fitur\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Inisiasi dan latih SVM\n",
    "            svm = SVC(\n",
    "                kernel='rbf',  \n",
    "                C=1.0,  \n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            # Validasi silang\n",
    "            cv_scores = cross_val_score(svm, X_train_scaled, y_train, cv=5)\n",
    "            \n",
    "            # Latih model\n",
    "            svm.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Prediksi\n",
    "            y_pred = svm.predict(X_test_scaled)\n",
    "            \n",
    "            # Buat laporan klasifikasi\n",
    "            report = classification_report(y_test, y_pred, output_dict=True)\n",
    "            \n",
    "            # Matriks konfusi\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            \n",
    "            # Visualisasi matriks konfusi\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title(f'Matriks Konfusi - {csv_file} ({method_name})')\n",
    "            plt.ylabel('Label Sebenarnya')\n",
    "            plt.xlabel('Label Prediksi')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(results_dir, f'confusion_matrix_{csv_file.replace(\".csv\", \"\")}_{method_name}.png'))\n",
    "            plt.close()\n",
    "            \n",
    "            # Simpan ringkasan\n",
    "            summary_entry = {\n",
    "                'Dataset': csv_file,\n",
    "                'Metode Seleksi Fitur': method_name,\n",
    "                'Akurasi': report['accuracy'],\n",
    "                'Rata-rata F1 Makro': report['macro avg']['f1-score'],\n",
    "                'Rata-rata Skor CV': cv_scores.mean(),\n",
    "                'Std Skor CV': cv_scores.std()\n",
    "            }\n",
    "            classification_summary.append(summary_entry)\n",
    "            \n",
    "            # Cetak hasil\n",
    "            print(f\"\\nHasil Klasifikasi untuk {csv_file} ({method_name}):\")\n",
    "            print(f\"Skor Validasi Silang: {cv_scores}\")\n",
    "            print(f\"Rata-rata Skor CV: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "            print(\"\\nLaporan Klasifikasi:\")\n",
    "            print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Simpan ringkasan ke CSV\n",
    "    summary_df = pd.DataFrame(classification_summary)\n",
    "    summary_df.to_csv(os.path.join(results_dir, 'classification_summary.csv'), index=False)\n",
    "    print(\"\\nRingkasan klasifikasi disimpan di classification_summary.csv\")\n",
    "\n",
    "def perform_rfe_selection(X, y, n_features_to_select=10):\n",
    "    \"\"\"\n",
    "    Lakukan Recursive Feature Elimination (RFE)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : DataFrame\n",
    "        Data fitur\n",
    "    y : Series\n",
    "        Label target\n",
    "    n_features_to_select : int, optional\n",
    "        Jumlah fitur yang akan dipilih\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame dengan fitur terpilih\n",
    "    \"\"\"\n",
    "    estimator = SVC(kernel=\"linear\")\n",
    "    selector = RFE(estimator, n_features_to_select=n_features_to_select)\n",
    "    selector = selector.fit(X, y)\n",
    "    return X.loc[:, selector.support_]\n",
    "\n",
    "def perform_pca_selection(X, n_components=10):\n",
    "    \"\"\"\n",
    "    Lakukan Principal Component Analysis (PCA)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : DataFrame\n",
    "        Data fitur\n",
    "    n_components : int, optional\n",
    "        Jumlah komponen utama yang akan dipilih\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame dengan komponen utama\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return pd.DataFrame(X_pca)\n",
    "\n",
    "# Direktori input dan output\n",
    "input_dir = 'output-baru/rfe_results'\n",
    "results_dir = 'output-baru/svm_classification_results'\n",
    "\n",
    "# Jalankan Klasifikasi SVM\n",
    "perform_svm_classification(input_dir, results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "def extract_numeric_features(df):\n",
    "    \"\"\"\n",
    "    Extract only numeric columns for classification\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with only numeric columns\n",
    "    \"\"\"\n",
    "    # Identify numeric columns\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remove 'Frame' column if it exists in numeric columns\n",
    "    if 'Frame' in numeric_columns:\n",
    "        numeric_columns.remove('Frame')\n",
    "    \n",
    "    return df[numeric_columns]\n",
    "\n",
    "def load_and_preprocess_data(csv_path):\n",
    "    \"\"\"\n",
    "    Load data from CSV and preprocess for SVM classification\n",
    "    \n",
    "    Parameters:\n",
    "    csv_path (str): Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X, y) where X is feature matrix and y is target labels\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Ensure label exists\n",
    "    if 'Label' not in df.columns:\n",
    "        raise ValueError(\"No 'Label' column found in the dataset\")\n",
    "    \n",
    "    # Extract only numeric features\n",
    "    X = extract_numeric_features(df)\n",
    "    \n",
    "    # Extract labels\n",
    "    y = df['Label']\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title, output_path):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix using matplotlib\n",
    "    \n",
    "    Parameters:\n",
    "    cm (array): Confusion matrix\n",
    "    classes (list): List of class labels\n",
    "    title (str): Title of the plot\n",
    "    output_path (str): Path to save the plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def perform_svm_classification(X, y, component_name):\n",
    "    \"\"\"\n",
    "    Perform SVM classification with grid search and cross-validation\n",
    "    \n",
    "    Parameters:\n",
    "    X (DataFrame): Feature matrix\n",
    "    y (Series): Target labels\n",
    "    component_name (str): Name of the facial component being classified\n",
    "    \n",
    "    Returns:\n",
    "    dict: Classification results\n",
    "    \"\"\"\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Define parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n",
    "        'kernel': ['rbf', 'linear', 'poly']\n",
    "    }\n",
    "    \n",
    "    # Create SVM classifier\n",
    "    svm = SVC(probability=True)\n",
    "    \n",
    "    # Perform Grid Search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=svm, \n",
    "        param_grid=param_grid, \n",
    "        cv=5, \n",
    "        n_jobs=-1, \n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit Grid Search\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Best model\n",
    "    best_svm = grid_search.best_estimator_\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = best_svm.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluation\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(best_svm, X_train_scaled, y_train, cv=5)\n",
    "    \n",
    "    # Visualization of Confusion Matrix\n",
    "    plot_confusion_matrix(\n",
    "        conf_matrix, \n",
    "        classes=np.unique(y), \n",
    "        title=f'Confusion Matrix - {component_name}', \n",
    "        output_path=f'output-baru/confusion_matrix_{component_name}.png'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'accuracy': accuracy,\n",
    "        'cross_val_scores': cv_scores,\n",
    "        'mean_cv_score': cv_scores.mean(),\n",
    "        'std_cv_score': cv_scores.std(),\n",
    "        'classification_report': class_report\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to perform SVM classification on different facial components\n",
    "    \"\"\"\n",
    "    # Path to CSV directory\n",
    "    csv_dir = 'output-baru/csv'\n",
    "    \n",
    "    # Results dictionary\n",
    "    classification_results = {}\n",
    "    \n",
    "    # Iterate through CSV files\n",
    "    for filename in os.listdir(csv_dir):\n",
    "        if filename.endswith('.csv') and not filename.startswith('nilai-fitur'):\n",
    "            # Full path to CSV\n",
    "            csv_path = os.path.join(csv_dir, filename)\n",
    "            \n",
    "            # Component name (without .csv)\n",
    "            component_name = os.path.splitext(filename)[0]\n",
    "            \n",
    "            print(f\"\\nProcessing classification for {component_name}\")\n",
    "            \n",
    "            # Load and preprocess data\n",
    "            try:\n",
    "                X, y = load_and_preprocess_data(csv_path)\n",
    "                \n",
    "                # Print dataset info for debugging\n",
    "                print(f\"Features shape: {X.shape}\")\n",
    "                print(f\"Labels shape: {y.shape}\")\n",
    "                print(f\"Unique labels: {y.unique()}\")\n",
    "                \n",
    "                # Perform classification\n",
    "                results = perform_svm_classification(X, y, component_name)\n",
    "                \n",
    "                # Store results\n",
    "                classification_results[component_name] = results\n",
    "                \n",
    "                # Print results\n",
    "                print(f\"\\nResults for {component_name}:\")\n",
    "                print(\"Best Parameters:\", results['best_params'])\n",
    "                print(\"Accuracy:\", results['accuracy'])\n",
    "                print(\"Cross-Validation Scores:\", results['cross_val_scores'])\n",
    "                print(\"Mean CV Score:\", results['mean_cv_score'])\n",
    "                print(\"Classification Report:\\n\", results['classification_report'])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {component_name}: {e}\")\n",
    "                # Optional: Print full traceback for debugging\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    # Save detailed results to a text file\n",
    "    with open('output-baru/classification_results.txt', 'w') as f:\n",
    "        for component, results in classification_results.items():\n",
    "            f.write(f\"\\n--- {component} Classification Results ---\\n\")\n",
    "            f.write(f\"Best Parameters: {results['best_params']}\\n\")\n",
    "            f.write(f\"Accuracy: {results['accuracy']}\\n\")\n",
    "            f.write(f\"Cross-Validation Scores: {results['cross_val_scores']}\\n\")\n",
    "            f.write(f\"Mean CV Score: {results['mean_cv_score']}\\n\")\n",
    "            f.write(f\"Standard Deviation of CV Scores: {results['std_cv_score']}\\n\")\n",
    "            f.write(f\"Classification Report:\\n{results['classification_report']}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFOLD Fitur All Component\n",
    "from clasification.svm import SVMClassifier\n",
    "\n",
    "# Path dataset\n",
    "dataset_file = 'output-baru/csv/nilai-fitur-all-component.csv'\n",
    "label_column = 'Label'\n",
    "except_feature_columns = ['Frame', 'Folder Path', 'Label']  # Kolom yang dikecualikan\n",
    "\n",
    "# Inisialisasi classifier\n",
    "classifier_kfold = SVMClassifier(dataset_file, label_column, except_feature_column=except_feature_columns)\n",
    "\n",
    "# Load data\n",
    "classifier_kfold.load_data()\n",
    "\n",
    "# Train model menggunakan k-fold cross validation\n",
    "classifier_kfold.train_model_cross_validation(cv=10)\n",
    "\n",
    "# Evaluate model menggunakan k-fold cross validation\n",
    "classifier_kfold.evaluate_model_cross_validation()\n",
    "\n",
    "# Save model\n",
    "classifier_kfold.save_model('svm_model_kfold.joblib', 'label_encoder_kfold.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Sampling 4QMV\n",
    "from clasification.svm import SVMClassifier\n",
    "\n",
    "# Load the dataset and initialize the classifier\n",
    "# dataset_file = 'test-output/4qmv-all-component.csv'\n",
    "# dataset_file = 'test-output/nilai-fitur-all-component.csv'\n",
    "# dataset_file = 'output/csv/4qmv-all-component.csv'\n",
    "dataset_file = 'output-baru/csv/all-component.csv'\n",
    "label_column = 'Label'\n",
    "except_feature_columns = ['Frame', 'Folder Path', 'Label']  # Columns to exclude\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier = SVMClassifier(dataset_file, label_column, except_feature_column=except_feature_columns)\n",
    "\n",
    "# Load, split, train, evaluate and save the model\n",
    "classifier.load_data()\n",
    "classifier.split_data(test_size=0.2)\n",
    "classifier.train_model(autoParams=True)\n",
    "classifier.evaluate_model()\n",
    "classifier.save_model('4qmv_svm_model_random_sampling.joblib', '4qmv_label_encoder_random_sampling.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Sampling 4QMV\n",
    "from clasification.svm import SVMClassifier\n",
    "\n",
    "# Load the dataset and initialize the classifier\n",
    "# dataset_file = 'test-output/4qmv-all-component.csv'\n",
    "# dataset_file = 'test-output/nilai-fitur-all-component.csv'\n",
    "# dataset_file = 'output/csv/4qmv-all-component.csv'\n",
    "# dataset_file = 'test-output/onsetoffset_tanpa_fear/4qmv-all-component.csv'\n",
    "dataset_file = 'output-baru/csv/4qmv-all-component.csv'\n",
    "label_column = 'Label'\n",
    "except_feature_columns = ['Frame', 'Folder Path', 'Label']  # Columns to exclude\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier = SVMClassifier(dataset_file, label_column, except_feature_column=except_feature_columns)\n",
    "\n",
    "# Load, split, train, evaluate and save the model\n",
    "classifier.load_data()\n",
    "classifier.split_data(test_size=0.2)\n",
    "classifier.train_model(autoParams=True)\n",
    "classifier.evaluate_model()\n",
    "classifier.save_model('4qmv_svm_model_kfold.joblib', '4qmv_label_encoder_kfold.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFOLD 4QMV\n",
    "from clasification.svm import SVMClassifier\n",
    "\n",
    "# Load the dataset and initialize the classifier \n",
    "# dataset_file = 'test-output/4qmv-all-component.csv'\n",
    "# dataset_file = 'test-output/nilai-fitur-all-component.csv'\n",
    "# dataset_file = 'test-output/onsetoffset_tanpa_fear/4qmv-all-component.csv'\n",
    "dataset_file = 'output-baru/csv/4qmv-all-component.csv'\n",
    "label_column = 'Label'\n",
    "except_feature_columns = ['Frame', 'Folder Path', 'Label']  # Columns to exclude\n",
    "\n",
    "# Inisialisasi classifier\n",
    "classifier_kfold = SVMClassifier(dataset_file, label_column, except_feature_column=except_feature_columns)\n",
    "\n",
    "# Load data\n",
    "classifier_kfold.load_data()\n",
    "\n",
    "# Train model menggunakan k-fold cross validation\n",
    "classifier_kfold.train_model_cross_validation(cv=10)\n",
    "\n",
    "# Evaluate model menggunakan k-fold cross validation\n",
    "classifier_kfold.evaluate_model_cross_validation()\n",
    "\n",
    "# Save model\n",
    "classifier_kfold.save_model('4qmv_svm_model_kfold2.joblib', '4qmv_label_encoder_kfold2.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
